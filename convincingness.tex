% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}


%\renewcommand\UrlFont{\color{blue}\rmfamily}

\begin{document}
%
\title{Automatic Explanation Quality Assessment in Online Learning Environments}
%
\author{Sameer Bhatnagar\inst{1} \and
Amal Zouaq\inst{1} \and
Michel C. Desmarais\inst{1} \and
Elizabeth Charles\inst{2}
}
%
\authorrunning{S. Bhatnagar et al.}

\institute{Ecole Polytechnique Montreal 
\email{\{sameer.bhatnagar,amal.zouaq,michel.desmarais\}@polymtl.ca}\and
Dawson College
\email{echarles@dawsoncollege.qc.ca}\\
}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
\textit{
	150-250 words
}

\keywords{Argument mining  \and Learnersourcing \and Peer Instruction}
\end{abstract}


\section{Introduction}

\section{Related Work}

\subsection{Learnersourcing \& Comparative Peer Assessment}
Ripple\cite{khosravi_ripple_2019}, AXIS\cite{williams_axis:_2016}
Juxtapeer\cite{cambre_juxtapeer:_2018}

\subsection{Argument Quality \& Convincingness}
\begin{itemize}
	\item \cite{habernal_which_2016} Predicting convincingness, reducing noise 
	in annotations by building an acyclic 
	argument graph
	\item \cite{simpson_finding_2018} Gaussian Process Preference Learning 
	\item \cite{toledo_automatic_2019} Assessment of argument quality, with a 
	dataset that has both individual scores 
	and pairwise-ranked data 
	\item \cite{gleize_are_2019} Evidence quality, predicted using a Siamese 
	network architecture 
	
\end{itemize}
  




\section{Methods}

\subsection{Data}
The dataset is comprised of pairs of student explanations for a particular 
answer choice to a given question. The first explanation is always the one 
written by the learner-annotator, while the second is an alternative which 
they either chose as more convincing, or not. The data is filtered so as 
to only keep observations where the explanations are within half a standard 
deviation in length of each other. To ensure internal reliability, we only keep 
observations where we have at least \input{data/MIN_RECORDS_PER_STUDENT} 
records per 
student, and \input{data/MIN_VOTES_PER_CHOSEN_RATIONALE} records per chosen 
explanation. This leaves us a dataset with \input{data/NUM_STUDENTS_FILTERED} 
learner annotators and \input{data/N} observations across three disciplines.
\begin{table}
\input{data/data}
\caption{Observations of students choosing a peer explanation as more 
convincing than their own, or not, aggregated by discipline and whether they 
started and finished with the correct answer}
\end{table}

\subsection{Models}
The first baseline model we compare to is where students simply choose the 
longer explanation of the pair, while the second is based solely on a Bag of 
Words model trained on all of the words used by students for this item, and the 
words in 


\section{Results}
\begin{table}
	\begin{tabular}{l|l|l}
		\toprule
		model & accuracy & AUC \\
		\toprule
		Argument Length &  &  \\
		SVM-R &  &  \\
		GPPL & & \\
		BiLSTM & & \\
		Siamese &  &  \\
		BERT & & \\	
	\end{tabular}
\end{table}
\section{Discussion}

\section{Future Work}

 \bibliographystyle{splncs04}
 \bibliography{MyLibrary}
\end{document}
