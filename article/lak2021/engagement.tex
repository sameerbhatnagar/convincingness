%%
%% This is file `engagement.tex',
%% The first command in your LaTeX source must be the \documentclass command.
\documentclass[sigconf]{acmart}
\usepackage{hyperref}
\usepackage{pgf}
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    \normalfont B\kern-0.5em{\scshape i\kern-0.25em b}\kern-0.8em\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmcopyright}
\copyrightyear{2021}
\acmYear{2021}
\acmDOI{10.000/0000.0000}

%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[LAK '21]{LAK '21: Learning 
Analytics \& Knowledge}{April 2021}{UCI,CA}
%\acmBooktitle{Woodstock '18: ACM Symposium on 
%Neural Gaze Detection,
%  June 03--05, 2018, Woodstock, NY}
%\acmPrice{15.00}
%\acmISBN{978-1-4503-XXXX-X/18/06}


%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%% end of the preamble, start of the body of the document source.
\begin{document}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Modelling Argument Quality in Technology-Mediated 
Peer-Instruction}

\author{Sameer Bhatnagar}
\author{Antoine Lefebvre-Brossard}
\author{Michel C. Desmarais}
\author{Amal Zouaq}
\email{{sameer.bhatnagar,antoine.lefebvre-brossard,michel.desmarais,amal.zouaq}@polymtl.ca}
\affiliation{%
  \institution{Ecole Polytechnique Montreal}
  \country{Canada}
}


\renewcommand{\shortauthors}{Bhatnagar, et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
\textbf{TO DO}
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.

\begin{CCSXML}
	<ccs2012>
	<concept>
	<concept_id>10010405.10010489.10010490</concept_id>
	<concept_desc>Applied 
	computing~Computer-assisted 
	instruction</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	<concept>
	<concept_id>10010147.10010178.10010179</concept_id>
	<concept_desc>Computing methodologies~Natural 
	language processing</concept_desc>
	<concept_significance>500</concept_significance>
	</concept>
	</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Applied computing~Computer-assisted 
instruction}
\ccsdesc[500]{Computing methodologies~Natural 
language processing}


%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Peer Instruction, Learnersourcing}


\maketitle

\section{Introduction}

Technology-mediated peer instruction \textit{(TMPI)} platforms 
\cite{charles_harnessing_2019}\cite{univeristy_of_british_columbia_ubc/ubcpi_2019}
expand multiple choice items into a two step process.
On the first step, students must not only choose an answer choice, but also 
provide an explanation that justifies their reasoning. 
On the second step, students are prompted to revise their answer choice, by 
taking into consideration a subset of explanations written by their peers for
another answer choice.
In the case that the student wants to keep their original answer choice, but 
may be unsure of their own explanation, they are also shown peer-explanations 
for their original answer choice. 
The student now has three options:
\begin{enumerate}
	\item Change their answer choice, by indicating which of their peer's 
	explanations was most convincing
	\item keep their answer choice, but \textit{change explanations} by 
	choosing one that is for the same answer as their own
	\item choose ``I stick to my own'', indicating that their own explanation 
	is best from amongst those that are shown.
\end{enumerate}

Whenever the student goes with either of the first two scenarios above, we 
frame this as ``casting a vote'' for the chosen peer explanation.

The design and growing popularity of TMPI is inspired by three schools of 
thought: firstly, prompting students to explain their reasoning is beneficial 
to their learning \cite{chi_eliciting_1994}. 
Second, classroom based \textit{Peer Instruction}\cite{crouch_peer_2001}, often 
mediated by automated response systems (e.g. clickers), has become a prevalent, 
and often effective component in the teaching practice of instructors looking 
to drive student engagement as part of an active learning experience 
\cite{charles_beyond_2015}. 
In discussing with peers \textit{after} they have formulated their own 
reasoning, students are engaged in a higher order thinking task from Bloom's 
taxonomy, as they evaluate what is the strongest argument, before answering 
again.
Thirdly, by capturing data on which explanations students find most convincing, 
TMPI affords teachers the opportunity to mitigate the ``expert blind spot'' 
\cite{nathan_expert_2001}, addressing student misconceptions they might not 
otherwise have thought of.

In many teaching contexts, however, teachers do not have the time to provide 
feedback to every student explanation for every question item. 
The feedback students receive is primarily based on the correctness of their 
first and second answer choices, not the \textit{explanations} they write and 
choose.

The data at hand in TMPI environments enables scaling up how much feedback that 
can given.
We suggest that the ``vote'' data collected on each explanation, is a proxy for 
argument quality, along the dimension of \textit{convincingness}, as judged by 
peer learners. 
These votes can be aggregated into a \textit{convincingness} score, as a 
measure of how effective that explanation is in persuading peers to change 
their own answer.
Instructors and students could benefit from analytics with respect to the most 
and least convincing explanations.

However, aggregating these student ``votes'' into a meaningful and reliable 
score of \textit{convincingness}, is not trivial. 
Students do not provide a holistic score of each explanation, but a comparative 
judgement of their chosen explanation, relative only to the subset that was 
shown to them on their review page.

We set out to examine different measures of rank aggregation methods that can 
be used to measure argument quality, along the dimension of 
\textit{convincingness}, and model their role in the TMPI process. 
Our specific research questions are:
\begin{itemize}
	\item[RQ1] What measures of argument \textit{convincingness} are best 
	suited to aggregating the ``vote'' data from TMPI?
	\item[RQ2] What are the features of student explanations that explain which 
	argument are ranked as most convincing?
\end{itemize}

We suggest that the results of our work can inform the design of TMPI 
platforms, in how feedback is generated by teachers, and presented to students. 
In a broader context, we aim to contribute the growing body of research 
surrounding peer-review contexts, specifically where students do not provide 
holistic scores, but generate their evaluative judgments in a comparative 
setting.
Peer-review platforms most often ask students to provide a score based on a 
rubric, but the difficulty lies in the ability of novices to generate useful 
feedback before they have gained expertise with the content. 
A growing number of peer-review platforms address this issue with pairwise
\textit{comparative} judgments.
Notable examples include ComPAIR\cite{potter_compair:_2017} and 
JuxtaPeer\cite{cambre_juxtapeer:_2018}, both of which present students with a 
pair of their peers' submissions, and evaluate them with respect to one another.




\section{Related Work}

\subsection{Learnersourcing student explanations}
This modality is a specific case of  
\textit{learnersourcing}\cite{weir_learnersourcing_2015}, wherein students first
generate content as part of their own learning process, that is ultimately used 
to help their peers learn as well.
Notable examples include PeerWise \cite{denny_peerwise:_2008} and RiPPLE 
\cite{khosravi_ripple_2019}, both of which have student generate learning 
resources, which are subsequently used and evaluated by peers as part of 
formative assessment activities.

One of the earliest efforts to leverage peer judgments of peer-written 
explanations specifically is from the AXIS system\cite{williams_axis:_2016}, 
wherein students solved a problem, provided an explanation for their answer, 
and evaluated explanations written by their peers.
Using a reinforcement-learning approach known as ``multi-armed bandits'', the 
system was able to select peer-written explanations that were rated as helpful 
as those written by an expert.
Our research follows from these studies in scaling to multiple domains, and 
focusing on how the vote data can be used more directly to model argument 
quality as judged by peers.

\subsection{Ranking Arguments for Quality}
Rank aggregation is the task of combining the preferences of multiple agents 
into a single representative ranked list.
It has long been understood that obtaining pairwise preference data may be 
less prone to error on the part of the annotator, as it is a simpler task than 
rating on scales with more gradations. 
(This is relevant in TMPI, since each student is choosing one explanation as 
the most convincing in relation to the subset of others that are shown.)
 
A classical approach for rank aggregation from pairwise preference data is 
using the Bradley-Terry model, which has been extended to incorporate the 
quality of contributions of different annotators in a crowdsourced setting when 
evaluating relative reading level in a pair passages \cite{chen_pairwise_2013}. 

When evaluating argument convincingness, one of the first approaches proposed 
is based on constructing an ``argument graph'', where a weighted edge is drawn 
from node A to node B for every pair where argument A is labelled as more 
convincing than argument B. 
After filtering example pairs that lead to cycles in the graph, PageRank scores 
are derived from this directed acyclic graph, and the PageRank 
scores of each argument are used as the gold-standard to rank for 
convincingness \cite{habernal_which_2016}.

More recently, a relatively simpler heuristic Win-Rate score has been shown to 
be competitive alternative, wherein the rank score of an argument is simply the 
(normalized) number of times that argument has been chosen as more convincing 
in a pair, divided by the number of pairs it appears in
\cite{potash_ranking_2019}.

Finally, a neural approach based on RankNet has recently yielded state of the 
art results. By joining two Bidirectional Long-Short-Term Memory Networks in a 
Siamese architecture, and appending a softmax layer to the output, 
\cite{gleize_are_2019} show that we can jointly model pairwise preferences and 
overall ranks publicly available datasets.

We will explore two of these options as part of our methodology in our rank 
aggregation step: the probabilistic Bradley-Terry model, and the simple 
heuristic scoring model. 
(We leave the neural approach for future work, as the additional work required 
to address make the models interpretable enough for the educational context is 
out of the scope of this study)


\section{Methodology}

We borrow our methodological approach from research in argument mining (AM), 
specifically related to modelling argument quality along the dimension of 
\textit{convincingness}.
A common approach is to curate pairs of arguments made in defence of the same 
stance on the same topic.
These pairs are then presented to crowd-workers, who label which of the two is 
more convincing. 
These pairwise comparisons can then be aggregated using rank-aggregation 
methods so as to produce a overall ranked list of arguments.
We extend this work to the domain of TMPI, and define prediction tasks that not 
only aim to validate this methodology, but help answer our specific research 
questions.

\subsection{Rank Aggregation}
The raw data emerging from a TMPI platform is tabular, in the form of 
student-item observations.
The fields include the item prompt, the student's \textit{first} answer choice, 
their accompanying explanation, the peer explanations shown on the review step, 
the student's \textit{second} answer choice, and the peer explanation they 
chose as most convincing (\verb|None| if they choose to ``stick to their own'').

\begin{enumerate}
	\item \textbf{win-rate}, defined as the ratio of times an explanation is 
	chosen to the number of times it was shown (we add a correction for 
	rationales that were never shown); 
	\item \textbf{BT} score, which is the argument ``quality'' parameter 
	estimated for each explanation, according to the \textit{Bradley-Terry} 
	model, where the probability of argument A being chosen over argument B is 
	given by 

	$$
	P(a>b) = 
	\frac{e^{\beta_a}}{e^{\beta_a}+e^{\beta_b}}
	$$
	where $\beta_i$ is the latent strength parameter of argument $i$. 
	
	We decompose each student-item observation into argument pairs, where the 
	chosen explanation is paired with each of the other shown ones, and the 
	pair is labelled with $y=-/+1$, depending on whether the chosen explanation 
	is first/second in the pair.   
	The latent argument strength parameters are estimated by maximizing the 
	log-likelihood of the $m$ explanation-pairs:
	$$
	\ell(\boldsymbol{\beta})=\sum_{i=1}^{m}\sum_{j=1}^{m} 
	[w_{ij}ln\beta_i-w_{ij}ln(\beta_i+\beta_j)]
	$$
	subject to $\sum_{i}\beta_i=0$.
\end{enumerate}

In order to evaluate these rank aggregation different scores, and address 
\textbf{RQ1}, we employ a time-series based cross-validation scheme:
at each timestep, we calculate the aggregated argument \textit{convincingness} 
scores from past students, and set out to predict:
\begin{enumerate}
	\item which arguments will be chosen as more convincing from the pairs 
	constructed for the current student?  
	\item will the current student choose a peer's explanation as more 
	convincing than their own, or not?
\end{enumerate}

For the latter binary classification task, we also include \textit{surface 
level} features as a baseline, including the word count of explanation that the 
current student wrote; word counts of explanations that were shown on the 
review step; the number of explanations shown that were much shorter, or much 
longer than than the student's own explanation; and finally whether the 
student's first answer is correct.

Once these \textit{surface level} and \textit{convincingness} features are 
calculated for each student explanation, related statistics are assembled as 
well with respect to the \textit{shown} explanations at each time step (e.g. 
maximum, minimum and mean \textbf{win rate} and \textbf{BT} scores of shown 
explanations, etc).

We build interpretable predictive models for this task, and inspect the 
relative feature importances in order to address our research questions from 
above.

We experiment with classification models that are interpretable, but limit our 
selection to the most linear (linear regression), to the most acceptably 
non-linear (Random Forests).


\subsection{Modelling Argument Quality Scores}
The goal \textbf{RQ1} is establish which rank aggregation methods are best 
suited for the context of TMPI, such that one can take the comparative 
preference data from many students who each see different subsets of peer 
explanations.
We build on the results from the previous section to now predict these 
aggregate scores for each explanation, using linguistic properties of the 
explanations.
We address \textbf{RQ2} with a regression task of predicting the argument 
\textit{convincingness} scores using two different approaches to representing 
the student text: as an embedding inside a vector space models, or via a 
feature-rich document vector.

We experiment with vector space models with different document representations:
\begin{enumerate}
	\item LSA vectors (10,50,100 components) \cite{deerwester_indexing_1990}
	\item Glove embeddings \cite{pennington_glove:_2014}
	\item BERT embeddings \cite{devlin_bert_2018}, out-of-the-box, and 
	fine-tuned for the current classification task
\end{enumerate}

The advantage of a feature-rich approach lies in the interpretability for 
teachers in their reporting tools, as well as generalizability to new items 
before vote data can be collected.
The list of features included here are derived from related work in argument 
mining \cite{habernal_which_2016}\cite{persing_end--end_2016}on student essays, 
automatic short answer scoring \cite{mohler_text--text_2009}

	\begin{itemize}
		
		\item Surface Features: 
		word count,
		sentence count, 
		max/mean word length, 
		max/mean sentence length;

		\item Lexical: 
		uni-grams \& bigrams, 
		type-token ratio, 
		number of keywords (defined by open-source discipline specific 
		text-book), 
		number of equations;
		
		\item Syntactic: 
		POS n-grams (e.g. \textit{nouns, prepositions, 
		verbs,conjunctions,negation, adjectives, adverbs, punctuation}), 
		modal verbs (e.g. \textit{must, should, can, might}),
		contextuality/formality measure \cite{heylighen_variation_2002},
		dependency tree depth;
		
		\item Semantic:
		using LSA vectors trained on domain specific corpora, in this case an 
		open-source textbook in the discipline, we calculate similarity to all 
		other explanations in LSA space;
		
		\item Co-reference \cite{persing_end--end_2016}: 
			fraction of entities from the prompt mentioned in each sentence, 
			averaged over all sentences (using neural Co-reference resolution)
			vector cosine similarity between student explanation and prompt, 
			and answer choices; 
		
		\item Readability:
			Fleish-Kincaid,
			Coleman-Liau,
			spelling errors
	\end{itemize}


Features typical to NLP analyses in Learning Analytics that are not included 
here are cohesion, sentiment, and psycholinguistic features.





\section{Data}

The data for this study come from myDALITE.org, which is a hosted instance of 
an open-source project, 
\verb|dalite|\footnote{\url{https://github.com/SALTISES4/dalite-ng}}, 
maintained by a Canadian researcher-practitioner partnership focused on 
supporting teachers develpping active learning pedagogy 
\href{saltise.ca}{SALTISE}.

Table \ref{tab:data_summary} gives an overview of the dataset included in this 
study.
The data is from introductory level university science courses, and generally 
spans different teachers at different colleges and universities in Canada. 
 
\begin{table}
	\input{data/data_summary_final}
	\caption{
		Summary statistics of data, aggregated by discipline. 
		The columns are a=number of answers, s=number of students, q=number of 
		items, $\overline{a/s}$=mean number of answers completed by each 
		student (with standard deviation), d=question difficulty, as defined by 
		overall success rate of choosing correct answer choice on first 
		attempt, and $\Delta$=the fraction of answers where students chose an 
		explanation other than their own on the review step. 
	}
	\label{tab:data_summary}
\end{table}

\section{Results}

\begin{figure}
	\scalebox{0.6}{\input{img/acc_by_rank_score_type.pgf}}
	\caption{
		Comparing the ability of different rank aggregation scores in predicting
		which argument is more convincing from a pair, under a time-series 
		cross-validation scheme, for different disciplinary subsets of TMPI 
		data.
	}
	\label{fig:acc_by_rank_score_type}
\end{figure}

\textbf{TO DO}

\section{Discussion}
\textbf{TO DO}


\section{Limitations and Future Work}
\textbf{TO DO}

\begin{enumerate}
	\item Students are not explicitly directed on how to evaluate their peers' 
	explanations. This may have an impact 
	 \url{https://link.springer.com/article/10.1007/s10734-017-0220-3}
\end{enumerate}

\begin{acks}
Funding for this research was made possible by the support of the Canadian 
Social Sciences and Humanities Research Council \textit{Insight} Grant. This 
project would not have been possible without the SALTISE/S4 network of 
researcher practitioners, and the students using myDALITE.org who consented to 
share their learning traces with the research community.
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{MyLibrary}

\end{document}
\endinput
%% End of file `engagement.tex'.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% ARCHIVE

%\subsection{Text-to-text similarity}
%Many of the features in our models are based on 
%similarity 
%metrics between 
%student explanations with each other, as well as 
%with other 
%references. 
%
%Consideration must be taken in choosing a metric 
%for measuring 
%similarity 
%between two texts. 
%The use of cosine similarity is standard practice 
%in Latent 
%Semantic 
%Indexing,
%
%\begin{equation}
%sim(T_1,T_2) = \frac{T_1 \cdot T_2}{\| T_1 \| \| 
%	T_2 \|}
%\end{equation}
%
%where $T_1$ and $T_2$ are the \textit{Tf-Idf} 
%vector 
%representations of 
%each text.
%
%However there is extensive work on text-to-text 
%similarity 
%metrics 
%\cite{mihalcea_corpus-based_2006}, 
%which can be divided into two families:
%\begin{enumerate}
%	\item Knowledge Based:
%	\item Corpus Based
%\end{enumerate}
%
%\subsection{Relative Learning Gain}
%Learning platforms used by teachers for 
%formative assessment 
%over the course of 
%a semester generate longitudinal learning 
%traces. 
%Fine grained modelling of student 
%learning with such data is 
%only possible if 
%question items have been tagged with 
%Knowledge Components\cite{corbett_knowledge_1994}, or an 
%item-skill 
%mapping\cite{barnes_q-matrix_2005} has been defined.
%Learning gains on the time-scale of a few 
%weeks, or a semester, 
%are often 
%measured using the administration of a 
%carefully validated 
%pre-post test, such 
%as the Force Concept Inventory\cite{hestenes_force_1992} from the 
%Physics Education 
%Research community.
%
%However both of these methodologies 
%require resource intensive 
%development of 
%domain knowledge mappings and validated 
%instruments. 
%In the iterative design of learning 
%tools, it is desirable to 
%have a definition 
%of learning that can be measured without 
%such prohibitive 
%limitations, even it 
%can serve as a baseline to more 
%comprehensive evaluations.
%
%We propose a definition of 
%\textit{Relative 
%	Learning Gain} for each student over 
%the course of a semester 
%which requires 
%only that all students within a group 
%have completed a common 
%set of items.
%\begin{equation}
%Relative Learning Gain_{student} = 
%\frac{W \rightarrow R_{second 
%		half of semester} }{W \rightarrow 
%	R_{first half of 
%		semester} }
%\end{equation}

%\begin{equation}
%Engagement_{student} = 
%\frac{1}{N_{explanations}}\sum{WordCount_{explanation}}
%\end{equation}




%Moreover, activities from online learning environments are often used for 
%formative assessment, and carry little weight in terms of course credit. 
%Framed as a low-stakes test, this can lead to low student motivation 
%\cite{wise_low_2005}. 
%The expectancy-value model \cite{pintrich_dynamic_1989}, which describes 
%factors that influence the effort students will direct towards a task, 
%includes 
%``how important they perceive the test to be'', and the ``affective reaction 
%to 
%how mentally taxing the task appears to be'' \cite{wolf_consequence_1995}.
%
%This further highlights the importance of providing feedback to students on 
%the 
%quality of their explanations, so as to emphasize the formative importance of 
%the writing activity and promote engagement.
