% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{pgf}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{fancyvrb}
\usepackage{todonotes}
\VerbatimFootnotes




\begin{document}
%
\title{Automatic Explanation Quality Assessment in Online Learning 
Environments: new datasets and methods}
\titlerunning{Convincingness \& Peer Instruction}
%
\author{Sameer Bhatnagar\inst{1} \and
Amal Zouaq\inst{1} \and
Michel C. Desmarais\inst{1} \and
Elizabeth Charles\inst{2}
}
%
\authorrunning{S. Bhatnagar et al.}

\institute{Ecole Polytechnique Montreal 
\email{\{sameer.bhatnagar,amal.zouaq,michel.desmarais\}@polymtl.ca}\and
Dawson College
\email{echarles@dawsoncollege.qc.ca}\\
}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
\textit{Asynchronous Peer Instruction} is increasingly popular in 
online learning environments. It relies on the principle of 
leveraging content that is both \textit{generated} and \textit{evaluated} by 
novices to foster learning and self-reflection. 
Students first respond to a question item, but 
they must also provide an explanation for their reasoning. 
They are then presented alternative explanations as written by their peers, and 
given the opportunity to change their initial answer choice, based on those 
they find most convincing. 
The peer-explanations that students find most convincing represent valuable 
data, for teachers to better grasp their students' understanding, and for the 
learning environment itself, as higher quality explanations can be shown to 
students as examples to compare their work to. 
This study reports on the application of text mining methods in the 
context of asynchronous peer instruction, with the objective of automatically 
identifying high quality student explanations. 
Our results offer the potential to inform the design of ``learnersourcing'' 
systems, such as asynchronous peer instruction.
These design choices are critical as these systems scale, especially with 
respect to providing pedagogically insightful reports to teachers, and 
presenting engaging alternative explanations to students to promote higher 
order thinking. 

\keywords{Argument mining  \and Learnersourcing \and Peer Instruction}
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\todo[inline]{Must be anonymized before submission}
\section{Introduction}

Learning environments that leverage peer submitted content, carry the great 
advantage of scaling up to a larger content base.  
Be it student-generated questions, sample solutions, feedback, or explanations 
and hints, peer-submitted content can easily help grow a database to meet the 
demand for practice items and formative assessment.
Platforms built on this principle are growing in 
popularity~\cite{denny_effect_2013}\cite{khosravi_ripple_2019}, freeing the 
teacher from the tedious task of developing multiple variants of the same 
items. 

But critical to the success of such environments is the capacity to 
automatically assess the quality of learner-generated content.
There are a growing number of tools which put students at the centre of this 
challenge of quality assessment
\cite{potter_compair:_2017}\cite{cambre_juxtapeer:_2018},
wherein after students submit their own work, after which they are prompted to 
evaluate a subset of their peers' submissions, and sometimes even provide 
feedback.

To counter the drawback that lies in the varying ability of novices to evaluate 
and provide good feedback to their peers, these environments often use 
\textit{pairwise} comparison. 
It is widely accepted that evaluative judgements based on ranking two objects 
relative to one another, are easier to make than providing an absolute score. 
Adaptive Comparative Judgement~\cite{pollitt_method_2012}, where teachers 
assess student submissions by simply choosing which is better from a pair, has 
been shown to be a reliable and valid alternative to absolute grading.
As such, there is a growing family of learning tools which, at the evaluation 
step, present the peer-generated content as \textit{pairs} to the current 
learner, and prompt for a pairwise ranking. 
This data can provide feedabck to the students who originally submitted the 
item, but can also be used for moderating database content.

In platforms where students generate content that is part of the learning 
activities of future students, filtering out irrelevant and misleading material 
is paramount. 
However, while removing bad content is important, educators hope to identify, 
and subsequently maximize the use of, the best student generated items.
Since not all students can be asked to evaluate all possible pairs, this in 
turn leads to the challenge of optimizing which items need evaluation by the 
``student-come-moderators'', without hindering their learning.
 
This is an example of the classic trade-off of exploration-vs-exploitation from 
the field reinforcement learning: how do we balance
\begin{itemize}
	\item exploiting the student 
	submissions for which we have reliable data, and can estimate their 
	quality, so that future students are assured of learning from this valuable 
	content, 
	\item while concurrently exploring the potential positive learning impact 
	of student contributions that are newer to the database, and need to be 
	shown and evaluated in order to get an estimate of their 
	quality?~\cite{williams_axis:_2016} 
\end{itemize}  
This challenge is a hallmark of learning environments that leverage 
\textit{learnersourcing}\cite{weir_learnersourcing_2015}.

The focus of this study is on the subset of these learning 
environments that enable \textit{peer instruction}\cite{crouch_peer_2001}, and 
are built on a specific two-stage script: 
\begin{enumerate}
	\item students are prompted to answer a multiple choice question, and 
	provide a free-text explanation that justifies their answer;
	\item without revealing the correct answer, students are then prompted to 
	reconsider their answer, by presenting them a selection of explanations 
	written by previous students~\cite{bhatnagar_dalite:_2016}.
	Students can either decide that their own explanation is best, or indicate 
	which of their peers' explanation was the most convincing.
\end{enumerate}
In this instance of \textit{learnersourcing}, as in the tools above, this 
``vote data'' is valuable at two levels: it can then be used to determine what 
to present to future students, but also inform instructors of their students' 
understanding of the material.

We frame the explanations students produce here as \textit{arguments} 
meant to persuade one's peers that their own reasoning is correct.
The ultimate objective of this study is to automatically assess the quality of 
these arguments, as they may help future students reflect on their own 
reasoning surrounding different concepts.
We borrow from work in the argument mining community, where argument quality 
can be measured along the dimension of \textit{convincingness}, as this gives 
us an operational definition: in a peer-instruction setting, can we predict 
which peer-explanations will be selected as more convincing than the student's 
own work? 

To our knowledge, this is the first analysis of peer instruction data through 
the lens of \textit{learnersourcing} and argument \textit{convincingness}.
Thus, as a reference, we include datasets and methods from the \textit{argument 
mining} research community specifically focused on pairwise preference ranking 
of arguments for automatic assessment of convincingness.
We apply vector space models, as well as state-of-the-art neural approaches, 
and report on their performance on this task.

Our findings suggest that the arguments generated in learning environments 
centred on undergraduate science topics present a more challenging variant of 
the task originally proposed in the argument mining community, and that 
classical approaches can match neural models for performance on this task 
across datasets, depending on the context.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}


\subsection{Learnersourcing \& Comparative Peer Assessment}
The term ``learnersourcing'' was coined by \cite{weir_learnersourcing_2015}, 
and defined as the process in ``which learners collectively generate useful 
content for future learners while engaging in a meaningful learning experience 
themselves.'' 
One of the earliest examples of a learning environment centred on this mode is  
Peerwise~\cite{denny_peerwise:_2008}, wherein students generate question items 
for the subjects they are learning, share them with their peers, so that others 
can use them for practice. 
Ripple~\cite{khosravi_ripple_2019} is a similarly built system, but they add a 
recommendation engine which adaptively selects which problems to suggest to 
which students.

Other tools leave the creation of question items to teachers, but call on 
students to generate and evaluate \textit{explanations} for the answers.
The AXIS~\cite{williams_axis:_2016} system prompts students to provide an 
generate explanation for their response to a short answer question, and then 
evaluate a similar explanation from one of their peers on a scale of 1-10 for 
\textit{helpfulness}. This rating-data drives the reinforcement learning 
algorithm that then decides which explanations to show to which future students.

A similar class of learning platforms leverage comparative judgement for 
supporting how students can evaluate the work of their peers. 
Juxtapeer~\cite{cambre_juxtapeer:_2018} asks students to provide feedback to a 
single peer on their work by explicitly comparing to that of another.
ComPAIR~\cite{potter_compair:_2017} asks students for feedback on each item of 
the pair they are presented with, with a focus on what makes one ``better'' 
than the other.

Finally, \textit{peer instruction} question prompts are being used more 
frequently inside online learning 
assignments~UBCPI~\cite{univeristy_of_british_columbia_ubc/ubcpi_2019}
\cite{charles_harnessing_2019}.
The student is presented with a multiple choice item, and prompted to provide 
an explanation of their answer choice. 
In order to encourage students to reflect on their own reasoning, they are then 
presented with a subset of alternative peer explanations to their own answer 
choice, as well for another choice. The student is then given the chance to 
revise their answer choice (or not), by indicating which of the peer 
explanations was most \textit{convincing} from the subset.


\subsection{Argument Quality \& Convincingness}
Conventional argument-mining pipelines include several successive components, 
starting with the automatic detection of argumentative units, classification of 
these units into types (e.g. major claim, minor claim, premise), and 
identification of argumentative relations (which evidence units support which 
claim). 
Such pipelines are essential in question-answering systems 
\cite{lippi_argumentation_2016} and are at the heart if the IBM Project Debater 
initiative. 

Work in the area of automatic evaluation of argument quality finds its roots in 
detecting evidence in legal texts~\cite{moens_automatic_2007}, but has 
accelerated in recent years as more datasets become available in everyday 
contexts, and focus shifts to modelling more qualitative measures, such as 
\textit{convincingness}. 

Some of the earlier efforts included work on automatically 
scoring of persuasive essays \cite{persing_end--end_2016} and modelling 
persuasiveness in online debate forums \cite{tan_winning_2016}. 
However, evaluating argument \textit{convincingness} with an absolute score can 
be challenging, which has led to significant work in adopting a pairwise 
approach, where data consists of pairwise observations of two arguments, 
labelled with which of the two is most convincing.

In \cite{habernal_which_2016}, the authors propose a feature-rich support 
vector machine, as well as an end-to-end neural approach based on pre-trained 
Glove vectors and a bidirectional Long-Short-Term Memory network for the 
pairwise classification task. 
This is extended in \cite{gleize_are_2019}, where the authors build a Siamese 
network architecture, where each leg is a BiLSTM, taking as input the pair of 
explanations as Glove embeddings \cite{pennington_glove:_2014}, in order to 
detect which of argument in a pair 
has the most convincing evidence.
Finally, based on the success of transformer models such as 
BERT~\cite{devlin_bert_2018}, the authors of \cite{toledo_automatic_2019}  
release a dataset of argument pairs and show that these models accurately 
predict the most convincing argument in a pair.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
\section{Data}

One of the objectives of this study is to compare and contrast how text 
mining methods for evaluating argument quality, specifically for argument 
\textit{convincingness}, perform in an online learning environment with 
learner-generated and annotated arguments. 
Our primary dataset comes from a \textit{peer instruction} learning 
environment, myDALITE.org. 
To provide context as to the performance that can be expected for this 
relatively novel task, we include in our study three publicly available 
datasets, each specifically curated for the task of automatic assessment of 
argument quality along the dimension \textit{convincingness}. 
Table \ref{tab:sample_obs} provides examples of an example of an argument paoir 
from each of the datasets.


\begin{table}
	\begin{subtable}[t]{\textwidth}
	\input{publication_artefacts/data/sample_obs_UKP}
	\end{subtable}
	\begin{subtable}[t]{\textwidth}
	\input{publication_artefacts/data/sample_obs_IBM}
	\end{subtable}
	\begin{subtable}[t]{\textwidth}
		\caption{Example of argument pair constructed from dalite 
				dataset}
		\input{publication_artefacts/data/sample_obs_dalite}

	\end{subtable}
	\caption{Examples of argument pairs from each dataset}
	\label{tab:sample_obs}
\end{table}



\subsection{UKP \& IBM}
\textbf{UKPConvArgStrict}\cite{habernal_which_2016}, hence forth referred to as 
\textbf{UKP}, was the first to propose the task of pairwise preference learning 
for argument convincingness. 
The dataset consists of just over 1k individual arguments, that support a 
particular stance for one of 16 topics. 
These arguments were distributed as 11.6k pairs to annotators on a 
crowd-sourcing platform, where the task was to choose which of the two 
arguments, for the same stance regarding the same topic, was more convincing. 

More recently, a second similar dataset, 
\textbf{IBMArgQ-9.1kPairs}\cite{toledo_automatic_2019}, henceforth 
referred to as \textbf{IBM\_ArgQ}, which is made of 3.4k individual arguments 
for 11 topics, assembled into 9.1k pairs labelled for which is more convincing. 
One of the key differences between these two is that \textbf{IBM\_ArgQ} data is 
more strongly curated with respect to the relative length of the arguments in 
each pair: in order to control for the possibility that annotators may make 
their choice of which argument in the pair is more \textit{convincing} based 
merely on the length of the text, the mean difference in word count, 
$\overline{\Delta wc}$, is just 3 words across the entire dataset, which is 10 
times more homogeneous than pairs in \textbf{UKP}.

Finally, we include a third reference dataset, \textbf{IBM\_Evi}, consisting of 
1.5k individual arguments, organized into 5.2k pairs annotated for pairwise 
preference for convincingness~\cite{gleize_are_2019}. The important distinction 
here is that the arguments are actually extracted as evidence for their 
respective topic from Wikipedia, and hence represent cleaner well-formed text 
than our other reference datasets.

Table \ref{tab:data_summary} summarizes some of the descriptive statistics that 
can be used to compare these sources, and potentially explain some of our 
experimental results. 

\begin{table}
	\caption{Descriptive statistics for each dataset of argument pairs, with 
		last rows showing \textbf{dalite} data split by discipline.$N_{args}$ 
		is the number of individual arguments, distributed across $N_{pairs}$ 
		revolving around $N_{topics}$. $\overline{wc}$ is the average number of 
		words per argument, shown with the standard deviation $(SD)$. 
		$\overline{\Delta wc}$ is the average relative difference in number of 
		words for each argument in each pair, shown with the standard deviation 
		.}
	\centerline{\input{publication_artefacts/data/df_summary_final}}
	\label{tab:data_summary}
\end{table}

\subsection{myDALITE}
A data point from a peer instruction item, will have the following fields: 
question prompt, answer choices, student's first answer choice, student's 
explanation for their first answer choice, the peer explanations they are shown 
on the review step, their second answer choice, and the explanation they found 
most convincing.
Quite often, students decide to keep the same answer choice, and indicate that 
their own explanation is te most convincing.
We construct our \textbf{dalite} dataset by keeping only the observations where 
students chose a peer's explanation as more convincing on the review step.   
Each argument pair is thus made up of the one written by the current learner, 
while the other is an alternative which they either chose as more convincing. 
In this respect, our peer-instruction dataset is different than the one 
mentioned above, since its is the same person, the student, who wrote the 
explanation, and as well as evaluated the its relative convincingness in the 
pair.

To ensure internal reliability, we only keep arguments that were chosen by
at least \input{publication_artefacts/data/VOTES_MIN}different students. 
To ensure that the explanations in each pair are of comparable length, we keep 
only those with word counts that are within 
\input{publication_artefacts/data/MAX_WORD_COUNT_DIFF}words of each other. 


This leaves us a dataset with \input{publication_artefacts/data/N}observations, 
spanning \input{publication_artefacts/data/n_students} learner annotators having 
completed, on average, 
\input{publication_artefacts/data/avg_q_per_student} items each, from a total of 
\input{publication_artefacts/data/n_questions} items across three disciplines, 
with at least 
\input{publication_artefacts/data/MIN_RECORDS_PER_QUESTION}explanation-pairs 
per item.


\begin{table}
	\caption{Observations of students choosing a peer explanation as 
		more 
		convincing than their own, or not, aggregated by discipline and whether 
		they 
		started and finished with the correct answer.}
	
	\centerline{\input{publication_artefacts/data/transitions_by_discipline}}
	
	\label{tab:transitions_by_discipline}
\end{table}

Table \ref{tab:transitions_by_discipline} highlights two key difference between 
the modelling task of this study, and related work in argument mining, where 
learner-annotators are presented pairs of arguments that are always for the 
same stance, in order to limit bias due to their opinion on the motion when 
evaluating which argument is more convincing (except in some observations of 
\textbf{IBM\_Evi}).
First, in a \textit{Peer Instruction} learning environment, other pairings are 
possible and pedagogically relevant. 
In this dataset, the majority of students keep the same answer choice between 
the two steps of the prompt, and so they are comparing two explanations that 
are either both correct (\textit{``rr''}) or incorrect (\textit{``wr''}). 
However, \input{publication_artefacts/data/frac_switch}\% of the 
observations in this dataset are for students who not only choose an 
explanation more convincing than their own, but also switch answer choice, 
either from the incorrect to correct, or the reverse. 
These pairs add a different level of complexity to the model, but are very 
pertinent in the pedagogical context: what are the argumentative features which 
can help students remediate an initial wrong answer choice (\textit{``wr''})?
What are the features that might be responsible for getting students to 
actually move away from the correct answer choice (\textit{``rw''})?
Second, a more fundamental difference is that our study focuses on 
undergraduate science courses across three disciplines, wherein the notion of 
answer ``correctness'' is important.
There are a growing number of ethics and humanities instructors using the peer 
instruction platform, where the question prompts are surrounding topics more 
like a debate, as in our reference datasets. 
We leave this comparison for future work.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methods}

%\todo[inline]{This section might be before the data sets and should clearly 
%name and describe the methods.....@Michel: with the new table describing State 
%of the Art performance of these methods on different datasets, i wonder if now 
%it is better to have methods after data?}
Choosing which argument is more convincing from a pair, is a binary 
ordinal regression task, where the objective is to learn a function that, given 
two feature vectors, can assign the better argument a rank of $+1$, and the 
other a rank of $-1$.   
It has been proven that such a binary ordinal regression problem, can be cast 
into an equivalent binary classification problem, wherein the model is trained 
on the \textit{difference} of the feature vectors of each argument in the pair 
\cite{herbrich_support_1999}. 
Referred to as \textit{SVM-rank}, this method of learning pairwise preferences 
has been used extensively in the context of information retrieval (e.g. ranking 
search results for a query based on past clickthrough data) 
\cite{joachims_optimizing_2002}, but also more recently in evaluating the 
journalistic quality of newspaper and magazine articles \cite{louis_what_2013}, 
and for predicting which argument is more convincing~\cite{habernal_which_2016}.

\subsection{Vector Space Models}

We follow-up on this work, building simple ``bag-of-words'' vector space models 
to represent our argument text. 
We take all of the individual arguments for a particular topic in our training 
set (known as  ``explanations'' for a particular question item in the case of 
the \textbf{dalite} data), lemmatize the tokens, and build term-document 
matrices for each topic.
We then take the arithmetic difference of these normalized term frequency 
vector representations of each argument to train Support Vector Machine 
classifiers on. 
We refer to this model as \textbf{ArgBow}. 
We do not, for this study, include any information related to the topic prompt 
in our document representation


\subsection{Pre-trained word embeddings}
A limitation of vectors space models is the exclusion of words that are 
``out-of-vocabulary'' between the test set and the training data.
This has led to experiments for this 
task\cite{habernal_which_2016}\cite{gleize_are_2019} with language models that 
offer pre-trained word-embeddings that have already learned from massive 
corpora of text.    
In our \textbf{ArgGlove} model, we encode each token of each argument using 
300-dimensional GloVe vectors~\cite{pennington_glove:_2014}, 
and represent each argument as the average of its token embedding vectors. 
We then feed these into the same SVM-rank architecture described above. 

\subsection{Transfer Learning}
Finally, in order to leverage recent advances in transfer-learning for NLP, the 
final model we explore is \textbf{ArgBert}.
We begin with a pre-trained language model for English built using  
Bi-directional Encoder Representation from Transformers, known as 
\textit{BERT}\cite{devlin_bert_2018}, trained on large bodies of text for the 
task of masked token prediction and sentence-pair inference. 
As proposed in \cite{toledo_automatic_2019}, we take the final 768-dimensional 
hidden state of the base-uncased BERT model, feed it into a binary 
classification layer, and fine-tune all of the pre-trained weights 
using our argument-pair data. 
As in other applications involving sentence pairs for BERT, each argument pair 
is encoded as \verb|[CLS] A [SEP] B]|, where the special \verb|[SEP]| token 
instructs the model as to which token belong to argument \verb|A| and which to 
argument \verb|B|. 
\footnote{modified from the \verb|run_glue.py| 
script provided by the \verb|tranformers| package, built by company hugging 
face. All code for this study provided in associated github repository}.

\subsection{Cross-Validation \& State of the Art}

The objective of this study is to apply methodology from the argument mining 
research community to inform the design of environment built upon 
learnersourcing of student explanations, so as to be able to be present student 
explanations that are \textit{convincing} to students as they engage with their 
reasoning when they answer conceptual questions.
As a baseline, we build a baseline model, \textbf{ArgLength}, which is trained 
on simply the number of words in each argument, as there may be many contexts 
where students will simply choose the longer/shorter argument, based on the 
prompt. 

In order to get a reliable estimate of performance, we employ stratified 5-fold 
cross-validation for our experiments.
This means that for each fold, we train our model on 80\% of the available 
data, ensuring that each topic and output class is represented equally. 
In the context of peer instruction learning environments, this is meant to give 
a reasonable estimate of how our models would perform in predicting which 
explanations will be chosen as most convincing, \textit{after} a certain number 
of responses have been collected. 

In Table \ref{tab:sota}, we denote, to the best of our knowledge, the 
state-of-the-art performance for each dataset on the task of pairwise 
classification for \textit{convincingness}. 
These values are meant to provide a reference as we present our results in the 
next section, but cannot be directly compared, as we employ a stratified 5-fold 
cross-validation scheme, while the reference studies measure cross-topic 
validation.

\begin{table} 
\centering\begin{tabular}{l|*{2}{c}r}
	Dataset     & Acc & AUC & model  	   \\
	\hline
	UKP 		& 0.83 & 0.89 & ArgBERT\cite{toledo_automatic_2019}    \\
	IBM\_ArgQ   & 0.80 & 0.86 & ArgBERT\cite{toledo_automatic_2019}    \\
	IBM\_Evi    & 0.73 & - 	  & EviConvNet\cite{gleize_are_2019} \\
\end{tabular}
\caption{State of the art performance for pairwise argument classification of 
convincingness for three publicly available datasets, using cross-topic 
validation scheme}
\label{tab:sota}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}

\begin{itemize}
	\item ArgLength sets baseline for each dataset. IBM datasets curate most 
	aggressively for pairs with comparable lengths.
	\item Vector space models performance comparable to BERT.
	\item GloVe most stable across datasets. UKP and IBMEvi performance on 
	ArgBoW due to wide-ranging vocabulary (lots of unknown tokens in test set), 
	which reveals limitation of approach 
\end{itemize}


\begin{figure}
	\begin{subfigure}[t]{0.5\linewidth}
		\centering
		\scalebox{0.5}{\input{publication_artefacts/img/acc.pgf}}
		\caption{Accuracy}
		\label{fig:acc_kfold}
	\end{subfigure}%
	\qquad
	\begin{subfigure}[t]{0.5\linewidth}
		\centering
		\scalebox{0.5}{\input{publication_artefacts/img/AUC.pgf}}
		\caption{RoC-AuC}
		\label{fig:AUC_kfold}
	\end{subfigure}

	\begin{subfigure}[t]{0.5\linewidth}
		\centering
		\scalebox{0.5}{\input{publication_artefacts/img/acc_dalite.pgf}}
		\caption{Accuracy}
		\label{fig:acc_dalite_kfold}
	\end{subfigure}%
	\qquad
	\begin{subfigure}[t]{0.5\linewidth}
		\centering
		\scalebox{0.5}{\input{publication_artefacts/img/AUC_dalite.pgf}}
		\caption{ROC-AUC}
		\label{fig:AUC_dalite_kfold}
	\end{subfigure}	
	\caption{Pairwise ranking classification accuracy and ROC-AUC for 
	different models in across datasets in figures \ref{fig:acc_kfold} and 
	\ref{fig:AUC_kfold}. Figures \ref{fig:acc_dalite_kfold} and 
	\ref{fig:AUC_dalite_kfold} split the performance for the \textbf{dalite} 
	dataset, across disciplines. All results evaluated using 5-fold stratified 
	cross-validation}.
	\label{fig:performance_k_fold}
\end{figure}

\begin{figure}
	\begin{subfigure}[t]{0.5\linewidth}
	\centering
	\scalebox{0.5}{\input{publication_artefacts/img/acc_cross_topic_val_True.pgf}}
	\caption{Accuracy}
	\label{fig:acc_cross_topic}
	\end{subfigure}%
\qquad
	\begin{subfigure}[t]{0.5\linewidth}
		\centering
		\scalebox{0.5}{\input{publication_artefacts/img/AUC_cross_topic_val_True.pgf}}
		\caption{ROC-AUC}
		\label{fig:AUC_cross_topic}
	\end{subfigure}
	\begin{subfigure}[t]{0.5\linewidth}
		\centering
		\scalebox{0.5}{\input{publication_artefacts/img/acc_dalite_cross_topic_val_True.pgf}}
		\caption{Accuracy}
		\label{fig:acc_dalite_cross_topic}
	\end{subfigure}%
\qquad
	\begin{subfigure}[t]{0.5\linewidth}
		\centering
		\scalebox{0.5}{\input{publication_artefacts/img/AUC_dalite_cross_topic_val_True.pgf}}
		\caption{ROC-AUC}
		\label{fig:AUC_dalite_cross_topic}
	\end{subfigure}
	\caption{Pairwise ranking classification accuracy and ROC-AUC for 
	different models in across datasets in figures \ref{fig:acc_cross_topic} 
	and \ref{fig:AUC_cross_topic}. Figures \ref{fig:acc_dalite_cross_topic} and 
	\ref{fig:AUC_dalite_cross_topic} split the performance for the 
	\textbf{dalite} dataset, across disciplines. All results evaluated using 
	cross-topic validation, where the number of folds equals the number of 
	topics}.
\label{fig:performance_cross_topic}

\end{figure}


\begin{itemize}
	\item performance of in chemistry worst, physics best, given their relative 
	proportions in data
	\item proportionately more errors for observations switch their answer 
	(check if true in IBM Evi, where stances can be different)
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

In \cite{louis_what_2013}, for task of pairwise ranking of newspaper articles 
based on ``quality'', the authors achieve a similar result: when comparing the 
performance of SVM-rank models using different input feature sets (e.g. 
\textit{use of visual language}, \textit{use of named entities}, 
\textit{affective content}), their top performing models achieve ``same-topic'' 
pairwise ranking accuracy of 0.84 using a combination of content and writing 
features, but also a 0.82 accuracy with the content words as features alone.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future Work}
\cite{nguyen_computational_2015} and \cite{louis_what_2013} use a combination 
of writing and content (BoW) features to achieve their best results, and thus 
this avenue must be explored more thoroughly, especially as this maybe vary 
across disciplines an teaching contexts. 
The state-of-the-art performance for pairwise classification accuracy on the 
\textbf{IBM\_Evi} dataset employs Siamese architechture\cite{gleize_are_2019}, 
where each leg of the network  is a Bi-directional Long-Short-Term-Memory 
network, with documents represented by word2vec embeddings. These variations 
are left to explore in future work.

In this study we do not ever infer which are, overall, the most convincing 
student explanations for any given item. Inferring a gold standard of global 
rankings, starting from these pairwise preference data can be accomplished 
using research from the information retrieval 
community~\cite{chen_pairwise_2013}. Work on deriving point wise scores for 
argument pairs is proposed as a Gaussian Process Preference Learning task by 
\cite{simpson_finding_2018}. Seeing the lack of pointwise labels for overall 
convincingness, \cite{toledo_automatic_2019} released a dataset where they 
collect this data as well. A comparable source of data inside the myDALITE 
platform are the feedback scores teachers can optionally provide to students on 
their explanations.


 \bibliographystyle{splncs04}
 \bibliography{MyLibrary}
\end{document}

%%%%%%%%%%%%%%%%%%
% ARCHIVE
%
%\begin{table}
%\begin{tabular}{ |l|l|l|}
%	\hline
%	Feature Type & Feature & $\tau$ \\
%	\hline
%	\multirow{6}{*}{Lexical} 
%	& Uni+BiGrams & \\
%	& Spelling Errors &  \\
%	& Equations & \\
%	& Type Token Ratio &  \\
%	& Punctuation &\\
%	& Readability scores&\\
%	\hline
%	\multirow{5}{*}{Syntactic} 
%	& PoS Uni+Bigrams &   \\
%	& Dependancy Tree Depth  & \\
%	& Conjunctions  & \\ 
%	& Modal Verbs & \\ 
%	& Tree Production Rules &\\
%	\hline
%	\multirow{3}{*}{Semantic} 
%	& LSA Similarity &  \\
%	& LSA Similarity Question & \\
%	& Likelihood (Textbook) & \\ 
%	\hline
%\end{tabular}
%
%\caption{Features used in experiments with Linear SVM, annotated with 
%	kendall $\tau$ correlation and sigificance with target label}
%\label{tab:features}
%\end{table}




%\begin{figure}
%	\subfloat[Baseline models on myDalite]
%	{	
%	\input{data/results_overall_myDalite}
%    }\hfill
%	\subfloat[Baselines on IBM argument ranking dataset]
%	{	
%	\input{data/results_overall_IBMPairs}
%    }
%	\label{tab:baselines}
%\end{figure}

%\begin{figure}
%	\subfloat[BERT model, fine tuned on myDalite data, by discipline]
%	{	
%		\input{data/BERT_dalite}
%	}\hfill
%	\subfloat[BERT model performance on Biology data, by answer transition]
%	{	
%		\input{data/BERT_Bio_transition}
%	}
%	\label{tab:BERT_dalite}
%\end{figure}

%\begin{figure}
%	\subfloat[BERT model, fine tuned on myDalite data, by discipline]
%	{	
%		\input{data/BERT_dalite}
%	}\hfill
%	\subfloat[BERT model performance on Biology data, by answer transition]
%	{	
%		\input{data/BERT_Bio_transition}
%	}
%	\label{tab:BERT_dalite}
%\end{figure}
%For our experiments, we begin by following the line of work proposed by 
%\cite{habernal_which_2016}, and experiment with a feature-rich linear SVM 
%classifier for the pairwise classification task. We use a similar feature set, 
%which we categorize as \textbf{lexical}, \textbf{syntactic}, and 
%\textbf{semantic}, as described in Table \ref{tab:features}. we begin by 
%computing the feature vector for each explanation, and compute the difference 
%for each pairwise ranking instance as per the well established SVM-Rank 
%algorithms \cite{joachims_optimizing_2002}, training the model to learn which 
%of the pair is the more convincing argument. 
%

%In pairwise classification tasks so 50\% is the baseline performance.  
%In Table \ref{fig:baselines} we begin by comparing the \textit{ArgLongest} 
%model baseline, where we predict that students simply choose the longer 
%explanation of the pair.
%We also include two baseline models on \textit{Bag of Words} models: 
%\textit{ArgBoWGen} where the term-document-matrix is built from an open-source 
%textbook from the corresponding discipline\footnote{https://openstax.org/}, 
%and 
%\textit{ArgBoWItemSpec}, where the term-document matrix is built from the 
%words 
%students have used for the item (pertinent when no reference text is available 
%for a discipline).
%As this is a new context for these argument mining methods, we include the 
%same 
%baselines on the carefully curated \textit{IBMArgPair} dataset, which is of 
%the 
%same format.
