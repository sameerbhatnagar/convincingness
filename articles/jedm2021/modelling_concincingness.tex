
\documentclass[notitlepage,12pt]{jedm}
%\usepackage[sc,sf,small]{titlesec}
\usepackage[table]{xcolor}
\usepackage{url}
\usepackage{hyperref}
\usepackage{pgf}
\usepackage{newtxmath}
\usepackage{booktabs}

\usepackage{ifthen}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{calc}
\usepackage{multirow}
\usepackage{graphicx,caption,subcaption}
\usepackage[finalnew]{trackchanges}


\hypersetup{
	colorlinks   = true, %Colours links instead of ugly boxes
	urlcolor     = blue, %Colour for external hyperlinks
	linkcolor    = blue, %Colour of internal links
	citecolor   = blue %Colour of citations
}

%-----------------------------------------------------------------------
% FINAL COPYEDITTED SUBMISSION - UNCOMMENT THIS TO SUPPRESS PAGE NUMBERS
%\pagenumbering{gobble}
%-----------------------------------------------------------------------

\begin{document}
	
	\title{Modelling Argument Quality in Technology Mediated Peer Instruction}
	\date{} %do not delete this, it suppresses insertion of the date
	
	\author{
		{\large Sameer Bhatnagar}
		\\Polytechnique Montreal
	 	\and 
	 	{\large Michel C. Desmarais}
	 	\\Polytechnique Montreal
	 	\and 
	 	{\large Amal Zouaq}
 		\\Polytechnique Montreal
 }

	
	\maketitle
	
	\begin{abstract}
		Technology Mediated Peer Instruction (TMPI) is the process whereby 
		students submit explanations to justify their reasoning, and are 
		subsequently prompted to reconsider their own answer by being presented 
		with explanations written by their peers. 
		We frame this as an instance of comparative judgment, as applied to 
		evaluating the quality of natural language arguments, along the 
		dimension of \textit{convincingness}.
		This study proposes a two-step methodology for modelling data from 
		TMPI: aggregation of pairwise preference data to produce rankings 
		ordered on quality (as judged by peers), followed by a regression task 
		using a rich set of linguistic features as input to supervised learning 
		algorithms that favour interpretability. 
		We evaluate this methodology on publicly available datasets from 
		argument mining research, and apply it to data from a TMPI learning 
		environment spanning data from multiple disciplines.		
		\\ %Keep \\ for spacing to keywords		
		{\parindent0pt
			\textbf{Keywords:} 
		}
	\end{abstract}

\section{Introduction}
Technology-mediated peer instruction \textit{(TMPI)} platforms 
\cite{charles_harnessing_2019}\cite{univeristy_of_british_columbia_ubc/ubcpi_2019}
expand multiple choice items into a two step process.
On the first step, students must not only choose an answer choice, but also 
provide an explanation that justifies their reasoning, as shown in figure 
\ref{fig:question_start}.

On the second step (figure \ref{fig:question_review}), students are prompted to 
revise 
their answer choice, by taking into consideration a subset of explanations 
written by their peers.

The student now has three options:
\begin{enumerate}
	\item Change their answer choice, by indicating which of their peer's 
	explanations for a \textit{different} answer choice was most convincing;
	\item keep the \textit{same} answer choice, but indicate which the peer's 
	explanations the student found more convincing than their own;
	\item choose ``I stick to my own'', which indicates that they are keeping 
	to the same answer choice, and that their own explanation is best from 
	among those that are shown.
\end{enumerate}

Whenever the student goes with either of the first two scenarios above, we 
frame this as ``casting a vote'' for the chosen peer explanation.

Moreover, when one of the answer choices is labelled at ``correct'', and the 
other are ``incorrect'', as is often the case in question items from the STEM 
disciplines, the three possibilities above can produce one of four 
\textit{transitions}: Right $\rightarrow$ Right, Right $\rightarrow$ Wrong, 
Wrong $\rightarrow$ Right, or Wrong $\rightarrow$ Wrong.
The transition possibilities, and the relative proportions present in the the 
TMPI platform we study, are shown in the Sankey diagram of figure 
\ref{fig:tmpi_sankey}.

\begin{figure}[H]
	\begin{subfigure}[b]{0.4\textwidth}
		\def\svgscale{0.50}
		\input{img/tmpi_question_start.pdf_tex}
		\caption{
			The first step in TMPI, where a student is presented with a 
			multiple choice item. The student must enter a ``rationale'', or 
			``explanation'' justifying their answer choice.
			\newline
			\newline
			The panel on the right, figure \protect\ref{fig:question_review}, 
			shows the second, review step of TMPI.
			Before any feedback is given on the correctness of their first 
			attempt, the student is prompted to reconsider their answer choice, 
			by reading a subset of explanations written by previous students.
			A set of peer-explanations is shown for the student's own answer 
			choice, and another set is shown for a different answer choice. 
		}
		\label{fig:question_start}
	\end{subfigure}
	\qquad
	\begin{subfigure}[b]{0.6\textwidth}
		\def\svgscale{0.50}
		\input{img/tmpi_question_review.pdf_tex}
		\caption{}
		\label{fig:question_review}
	\end{subfigure}
	\caption{The two steps in technology-mediated peer 
		instruction (TMPI)}
	\label{fig:tmpi}
\end{figure}



\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{img/transitions_final.png}
	\caption{The possible transition types that can occur in TMPI for student 
	answers between their first attempt (when they write their own 
	explanation), and the review step (when they are presented with peer 
	explanations). 
	The relative proportion of each transition type is shown in this Sankey 
	diagram for data from myDALITE.org}
	\label{fig:tmpi_sankey}
\end{figure}


The design and growing popularity of TMPI is inspired by three schools of 
thought: firstly, prompting students to explain their reasoning is beneficial 
to their learning \cite{chi_eliciting_1994}. 
Deliberate practice of argumentation in defence of one's ideas has been shown 
to improve informal reasoning for science students \cite{venville_impact_2010}.
There exists empirical evidence on the positive relationship between 
constructing formally sound arguments and deep cognitive elaboration, as well 
as individual acquisition of knowledge \cite{stegmann_collaborative_2012}.

Second, classroom based \textit{Peer Instruction} \cite{crouch_peer_2001}, 
often mediated by automated response systems (e.g. clickers), has become a 
prevalent, and often effective component in the teaching practice of 
instructors looking to drive student engagement as part of an active learning 
experience \cite{charles_beyond_2015}. 
In discussing with peers \textit{after} they have formulated their own 
reasoning, students are engaged in a higher order thinking task from Bloom's 
taxonomy, as they evaluate what is the strongest argument, before answering 
again.

Thirdly, by capturing data on which explanations students find most convincing, 
TMPI affords teachers the opportunity to mitigate the ``expert blind spot'' 
\cite{nathan_expert_2001}, addressing student misconceptions they might not 
otherwise have thought of.

We situate student explanations from TMPI, in the context of computational 
argumentation, a sub-field of NLP focused on identifying argumentative 
components, and in their links to one another.
Modelling argument ``quality'' is an area of active research, with direct 
applications in education, such as in automated scoring of 
persuasive essays written by students \cite{persing_modeling_2015} 
\cite{nguyen_argument_2018}.
When students are asked to debate in dyads, and prompted to either find 
consensus, or instead persuade their peers, there is a relationship between  
knowledge acquisition, and the quality of arguments the students produce, as 
measured by the presence of formal argumentative structures (e.g. claims, 
premise, etc.) \cite{garcia-mila_effect_2013}.

However experiments have also shown that the perceived quality of an argument 
can depend on the audience \cite{mercier_why_2011}, and so we adopt a more 
pragmatic measure of argument quality, centred on the premise that the goal of 
argumentation is persuasion.

In a comprehensive survey of research on the assessment of argument quality, 
\cite{wachsmuth_computational_2017} outline a taxonomy of major quality 
dimensions for natural language, with three principal aspects: logic, rhetoric, 
and dialect. 
As students vote on their peer's explanations in TMPI, they may be evaluating 
the logical cogency (e.g. is this argument sound?), or its rhetorical quality 
(e.g. is this argument phrased well?). 
We focus our work on students who choose a peer's explanation \textit{as more 
convincing than their own}, as there exists a significant bias for the option 
``I stick to my own''.

Therefore, we suggest that the ``vote'' data collected for each student's 
explanation in TMPI, is a proxy for argument quality, along the dimension of 
\textit{convincingness}, as judged by peer learners. 
This is a direct application of the argument mining (AM) task originally 
proposed by \cite{habernal_which_2016}: if crowd-workers are presented with a 
pair of arguments for the same stance of a debatable topic, can we predict 
which of the two they will choose as more convincing?
This task has already been extended to TMPI in previous work, wherein the 
objective is to predict which explanations students will choose as more 
convincing than their own \cite{bhatnagar_learnersourcing_2020}.

Student votes in TMPI can be aggregated into a \textit{convincingness} score, 
as a measure of how effective that explanation is in persuading peers to change 
their own answer.
Student explanations can then be ranked along such a score, allowing for 
instructors to gain insights on the thinking of their students with respect to 
specific content, and potentially even help students to improve how they 
communicate ideas within their discipline.
However aggregating these votes should be done with care: when a student 
chooses an explanation as convincing, they are doing so only with respect to 
the subset that were shown, as well as the one they wrote themselves.

The problem of aggregating the results of evaluative peer-judgments extends  
beyond TMPI.
For example, in response to the difficulty students can have providing a 
holistic score to their peers' work, there is a growing number of peer-review 
platforms built on \textit{comparative} judgments.
Notable examples include ComPAIR \cite{potter_compair:_2017} and 
JuxtaPeer \cite{cambre_juxtapeer:_2018}, both of which present students with a 
just a pair of their peers' submissions, and prompt the learner to evaluate 
them with respect to one another.
As in TMPI, students apply a comparative judgment to only the subset of peer 
content that they are shown during the review step.
There is a need for a principled approach to aggregating this learnersourced 
data, in a pedagogically relevant manner, despite the inevitable absence of 
some ``true'' ranking.

This sets the stage for our central research questions: 
\begin{itemize}
	\item[RQ1] since each student's ``vote'' in this context represents an 
	incomplete evaluative judgement, which rank aggregation 
	methods are best suited for ranking the quality of student 
	explanations in TMPI?
	\item[RQ2] once we establish a ranked list of explanations along the 
	dimension of \textit{convinciness}, can we model this construct, and 
	identify the linguistic features of the most effective student 
	explanations, as judged by their peers?
\end{itemize}

Work on modelling \textit{convincingness} has, in large part, been centred on 
web discourse data.
In the educational setting, previous work in automated scoring of persuasive 
essays has focused on modelling holistic scores given by \textit{experts} on 
longer form essays.
To our knowledge, we are among the first to aggregate and model student 
``votes'', in order to evaluate student explanations for their 
\textit{convincingness} as judged by \textit{peers}.

We suggest that the results of our work can inform the design of TMPI platforms.
However, in a broader context, we aim to contribute to the growing body of 
research surrounding technology-mediated peer-review, specifically where 
learners do not provide holistic scores, but generate their evaluative 
judgments in a comparative setting. 
Such platforms will invariably have to deal with at least three issues, which 
our work helps to address.

The first issue is about students: providing feedback to learners on the 
characteristics common to the most convincing arguments in their discipline, 
promotes learning and the development of critical reasoning skills.

The second issue is in providing support to teachers: in such platforms, the 
amount of data generated scales very quickly.
The data associated with each student-item pair includes many relevant 
variables: correct answer choice on first attempt, student explanation, subset 
of explanations shown, time spent writing and reading explanations, correct 
answer on second attempt, and the peer-explanation chosen as most convincing 
(see figure \ref{fig:make_pairs_a}).   
This amount of information can be overwhelming for instructors who use such 
tools regularly as part of formative assessment. 
Automatically identifying the highest, and lowest, quality student 
explanations, as judged by other students, can support instructors in providing 
timely feedback. 

A third related issue is in maintaining the integrity of such platforms: 
automatic filtering of irrelevant/malicious student explanations is paramount, 
since they may be shown to future students \cite{gagnon_filtering_2019}, a 
non-trivial task for natural language content, without expensive expert 
moderation.

This paper begins with an overview of related research in learnersourcing of 
student explanations, automatic short-answer grading, and argument quality 
ranking (section \ref{sec:related_work}).
We then describe our TMPI dataset, as well as publicly available reference 
datasets of argument quality, which we use to evaluate our methodology (section 
\ref{sec:datasets}).
Our most important contribution is in proposing a methodology for evaluating 
the quality of student explanations, along the dimension of 
\textit{convincingness}, in TMPI environments; we demonstrate this methodology 
in section \ref{sec:methodology} and propose evaluation metrics based on 
practical issues in TMPI environments.
Finally, we describe how we \textit{model} these convincingness ``scores'' so 
as to identify the linguistic features of explanations most often associated 
with high-quality explanations (section \ref{sec:model_results}).


\section{Related Work}\label{sec:related_work}

\subsection{Learnersourcing student explanations}
TMPI is a specific case of  \textit{learnersourcing} 
\cite{weir_learnersourcing_2015}, wherein students first generate content, and 
then help curate the content base, all as part of their own learning process.
Notable examples include PeerWise \cite{denny_peerwise:_2008} and RiPPLE 
\cite{khosravi_ripple_2019}, both of which have students generate learning 
resources, which are subsequently used and evaluated by peers as part of 
formative assessment activities.

One of the earliest efforts specifically leveraging peer judgments of 
peer-written explanations, is from the AXIS system \cite{williams_axis:_2016}, 
wherein students solved a problem, provided an explanation for their answer, 
and evaluated explanations written by their peers.
Using a reinforcement-learning approach known as ``multi-armed bandits'', the 
system was able to select peer-written explanations that were rated as helpful 
as those written by an expert.
The novel scheme proposed by \cite{kolhe_peer_2016} also applies the potential 
of learnersourcing to the task of short answer grading: the short answers 
submitted by students are evaluated by ``future'' peers who are presented with 
multiple choice questions, where the answer options are the short answers 
submitted by their ``past'' counterparts.
Our research follows from these studies in scaling to multiple domains, and 
focusing on how the vote data can be used more directly to model argument 
quality as judged by peers.



\subsection{Automated Writing Evaluation}

A central objective of our work is to evaluate the quality of student 
explanations in TMPI.
Under the hierarchy of automated grading methods proposed by  
\cite{burrows_eras_2015}, this task falls under the umbrella of automatic 
short-answer grading (ASAG); students must recall knowledge and express it 
in their own way, using natural language, using typically between 10-100 words. 
Their in-depth historical review of ASAG systems describes a shifting focus in 
methods, from matching patterns derived from answers written by experts, to 
machine-learning approaches, where n-grams and hand-crafted features are 
combined as input to supervised learning algorithms, such as decision trees and 
support vector machines.

For example, \cite{mohler_learning_2011} measure alignment between dependency 
parse tree structures of student answers, with those of an expert answer.
These alignment features are paired with lexical semantic similarity features 
that are both knowledge-based (e.g. using WordNet) and corpus-based (e.g. 
Latent Semantic Analysis), and used as input to support vector machines which 
learn to automatically grade short answers.

Another similar system proposed by \cite{sultan_fast_2016} starts with features 
measuring lexical and contextual alignment between similar word pairs from 
student answers and a reference answer, as well as semantic vector similarity 
using ``off-the-shelf'' word embeddings.  
They then augment their input with  ``domain-specific'' term-frequency and 
inverse document-frequency weights, to achieve their best results on several 
ASAG datasets using various validation schemes.
 
In addition to similarity features based on answer text, \cite{zhang_deep_2016} 
show that question-level (e.g. difficulty, expert-labelled knowledge 
components) and student-level features (e.g. pre-test scores, Bayesian 
Knowledge Tracing probability estimates) can improve performance on the ASAG 
task when input to a deep learning classifier.

While modelling the quality of TMPI explanations has much in common with the 
ASAG task, and can benefit from the features and methods from the systems 
mentioned above, a fundamental difference lies in how similarity to an expert 
explanation may not be the only appropriate reference.
The ``quality'' we are measuring is that which is observed by a group of peers, 
which may be quite different from how a teacher might explain a concept.

\subsection{Ranking Arguments for Quality}\label{sec:related_work:arg_quality}

Previous work on automated evaluation of long-form persuasive essays 
\cite{ghosh_coarse-grained_2016}, \cite{klebanov_argumentation_2016} 
\cite{nguyen_argument_2018} has focused on modelling the holistic scores given 
by experts.
Our work here does not set out to ``grade'' student explanations, but provide a 
ranked list for \textit{convincingness} as judged by a set of peers.

We cast this as a task in rank aggregation, with the objective combining the 
preferences of multiple agents into a single representative ranked list.
It has long been understood that obtaining pairwise preference data may be 
less prone to error on the part of the annotator, as it is a simpler task than 
rating on scales with more gradations.
The trade-off, of course is the quadratic scaling in the number of pairs one 
can generate. 
This is relevant in TMPI, since each student is choosing one explanation as 
the most convincing only in relation to the subset of others that are shown, 
and the potential permutations of explanations different students may see is 
intractably large for a typical question answered by 100+ students.

A classical approach specifically proposed by \cite{raman_methods_2014} for 
ordinal peer grading data is the Bradley-Terry (BT) model.
The BT model \cite{bradley_rank_1952} for aggregating pairwise preference data 
into a ranked list, assumes that predicting the winner of a pairwise 
``match-up'' between any two items is associated with the difference in the 
latent ``strength'' parameters for those two items, and these parameters can be 
calculated using maximum likelihood estimation.

The BT method has been extended to incorporate the quality of contributions of 
different annotators in a crowdsourced setting when evaluating relative reading 
level in a pair passages \cite{chen_pairwise_2013}. 

Specifically in the context of evaluating argument convincingness from pairwise 
preference data, one of the first approaches proposed is based on constructing 
an ``argument graph'', where a weighted edge is drawn from node A to node B for 
every pair where argument A is labelled as more convincing than argument B. 
After filtering passage pairs that lead to cycles in the graph, PageRank scores 
are derived from this directed acyclic graph, and are used as the gold-standard 
rank for convincingness \cite{habernal_which_2016}.
(This dataset is included in our study, from now on labelled as \textbf{UKP}.)

More recently, a relatively simpler heuristic WinRate score has been shown to 
be a competitive alternative for the same dataset, wherein the rank score of an 
argument is simply the (normalized) number of times that argument has been 
chosen as more convincing in a pair, divided by the number of pairs it appears 
in \cite{potash_ranking_2019}.

Finally, a neural approach based on RankNet has recently yielded state of the 
art results, by joining two Bidirectional Long-Short-Term Memory Networks in a 
Siamese architecture. By appending a softmax layer to the output, pairwise 
preferences and overall ranks were jointly modelled in publicly available 
datasets \cite{gleize_are_2019}.
(This dataset is also included in our study as a reference, labelled as 
\textbf{IBM\_Evi}.)

The key difference between to keep in mind between the above mentionned studies 
in modelling the quality rankings of arguments, and that of TMPI explanations, 
is that the students are not indifferent crowd-labellers: each student will 
have just submitted their own explanation justifying their answer choice, and 
we analyze the aggregate of their choices as they indicate when a peer may have 
explained something better than themselves.

We will explore two of these options as part of our methodology in our rank 
aggregation step, via several related methods: the probabilistic Bradley-Terry 
model, one of its variants (the Elo rating system), and the simple heuristic 
scoring model, ``WinRate''.
(We omit a neural approach in this study, as we consider the work on 
interpreting the model results from a neural model for pedagogical purposes, 
out of the scope of this paper.
The methods we chose have several readily available implementations in 
different programming languages, and we err on the side of simplicity when 
possible in our methodological choices.)



\section{Data}\label{sec:datasets}

\subsection{Argument Mining Datasets}
Much of our methodology is inspired by work on modelling argument quality along 
the dimension of \textit{convincingness}, as described in section 
\ref{sec:related_work:arg_quality}. 
In order to contextualize the performance of these methods in our educational 
setting, we apply the same methods to publicly available datasets from the 
AM research community as well, and present the results. 
These datasets are described in table \ref{tab:data_summary}, alongside the 
TMPI data at the heart of our study. 

The \textbf{UKP} dataset \cite{habernal_which_2016} is one of the first set of 
labelled argument pairs to be released publicly.
Crowd-workers were presented with pairs of arguments on the same stance of a 
debate prompt, and were asked to choose which was more convincing.
The authors of the \textbf{IBM\_ArgQ} dataset \cite{toledo_automatic_2019} 
offer a dataset that is similarly labelled, but much more tightly curated, with 
strict controls on argument word count and relative difference in lengths in 
each pair.
This was partly in response to the observation that crowd labels could often be 
predicted simply by choosing the longer text from the pair.
The labelled argument pairs in the \textbf{IBM\_Evi} dataset 
\cite{gleize_are_2019} are actually generated by scraping Wikipedia, and the 
crowd workers were asked to choose the argument from the pair that provided the 
more compelling evidence in support of the debate stance.

As described above in our section on related work, these datasets were released 
not just with the labelled argument pairs, but holistic rank scores for each 
argument, that were each derived in different ways. 
We will be comparing our proposed \textit{measures} of convincingness to these 
rank scores in section \ref{sec:methodology_eval}.


\subsection{DALITE}\label{sec:dataset_dalite}
The central data for this study come from myDALITE.org, which is a hosted 
instance of an open-source project, 
\verb|dalite|\footnote{\url{https://github.com/SALTISES4/dalite-ng}}, 
maintained by a Canadian researcher-practitioner partnership, 
\href{saltise.ca}{SALTISE}, focused on supporting teachers in the development 
of active learning pedagogy.
The data comes from introductory level university science courses, and 
generally spans different teachers at different colleges and universities in 
Canada. 
The \textit{Ethics} dataset comes from a popular MOOC, wherein the TMPI prompts 
are slightly different from the \textit{Physics} and \textit{Chemistry} 
prompts, in that there is no ``correct'' answer choice, and that the goal is to 
have students choose a side of an argument, and justify their choice.
Table \ref{tab:data_summary} gives an overview of the datasets included in this 
study.


\begin{table}
	\input{data/df_summary_all_datasets}
	\caption{
		Summary statistics for reference datasets from argument mining research 
		community, and DALITE, a TMPI environment used mostly in undergraduate 
		science courses in Canada. 
		In the argument reference datasets \textit{topic} are debate prompts 
		shown to crowdsourcing workers (e.g. \textit{``social media does more 
		good than harm''}), while a \textit{topic} in DALITE is a question item.
		The explanations given by students are analagous to the ``arguments'',  
		which are then assembled into pairs based on what was shown, and 
		eventually chosen by each student.
		\textit{wc} is the average number of tokens in each 
		argument/explanation in each topic.
		All averaged quantities are followed by a standard deviation in 
		parentheses.
	}
	\label{tab:data_summary}
\end{table}

To stay consistent with the argument mining reference dataset terminology, we 
refer to a question-item as a ``topic''.
Student explanations from DALITE are divided up by the associated question item 
prompts.
The transformation of TMPI student explanations (``args'') into ``pairs'' is 
described in section \ref{sec:methodology}. 
The filtering of DALITE data is based on the following three steps:
\begin{itemize}
	\item approximately 1 in 10 students decide that they want their 
	explanations to be shared with only with their instructor, and not seen by 
	other students, nor used for the purposes of research.
	The answers of these students are removed from the dataset. 
	\item There is no simple and reliable way to determine whether students 
	choose this option ``genuinely'' (because the shown alternatives were not 
	sufficiently convincing), or because they did not want to read their peers' 
	explanations.
	For this reason, we only include observations where students explicitly 
	change explanations (whether for their own answer choice, or for a 
	different answer choice, regardless of correctness.) 
	There is a strong bias for students to simply choose `\textit{I stick to my 
	own rationale}, and so this reduces our data by approximately 50\%.
	\item Many question items have been completed by several hundreds of 
	students. 
	As such, almost half of all student explanations have only been shown to 
	another peer; thus we retain only those student answers that have been 
	presented to at least 5 other students.
	\item As a platform for formative assessment, not all instructors provide 
	credit for the explanations students write, and there are invariably some 
	students who do not put much effort into writing good explanations.
	We include only those student answers that have at least 10 words.
	\item after the previous two steps, we only include data from those 
	questions that have at least 100 remaining student answers.
	\item we remove any duplicate pairs before the rank aggregation step that 
	have the same ``winning'' label, as explanations that appear earlier on in 
	the lifetime of a new question are bound to be shown more often to future 
	students.
\end{itemize}

We see in table \ref{tab:data_summary} that the resulting datasets from the 
different disciplines in our TMPI dataset are comparable to the reference AM 
datasets (just proportionately larger).
The division of the TMPI data into multiple disciplines, despite the source the 
same platform, is because we assume that in modelling the quality of 
explanations, different features will be important for each.

\section{Methodology}\label{sec:methodology}

We borrow our methodological approach from research in argument mining, 
specifically related to modelling argument quality along the dimension of 
\textit{convincingness}.
A common approach is to curate pairs of arguments made in defence of the same 
stance on the same topic.
These pairs are then presented to crowd-workers, whose task it is to label 
which of the two is more convincing. 
The pairwise comparisons can then be aggregated using rank-aggregation 
methods so as to produce a overall ranked list of arguments.
We extend this work to the domain of TMPI, and define prediction tasks that not 
only aim to validate this methodology, but help answer our specific research 
questions.

\subsection{Rank Aggregation}\label{sec:rank_agg}
The raw data emerging from a TMPI platform is tabular, in the form of 
student-item observations.
As shown in figure \ref{fig:make_pairs_a}(a), the fields include the item 
prompt, the student's \textit{first} answer choice, their accompanying 
explanation, the peer explanations shown on the review step (as in figure 
\ref{fig:question_review}), the student's 
\textit{second} answer choice, and the peer explanation they chose as most 
convincing (\verb|None| if they choose to ``stick to their own''), as well as 
timestamps for the first and second attempt.

After the filtering steps described above, we take the TMPI observations for 
each question, and construct explanation pairs, as in figure 
\ref{fig:make_pairs_a}(b).

\begin{figure}[H]
	\centering
	\def\svgscale{0.40}
	\input{img/make_pairs_a.pdf_tex}
	\caption{
	Example of student-item observations from a TMPI environment. 
	This figure follows from figure \protect\ref{fig:tmpi}.
	(a) Student $s_{30}$ chose the correct \textbf{D} as the answer on 
	their first attempt, and provided the explanation $e_{30}$ in the 
	dataset for this question. 
	The student is shown a subset of explanations from previous students for 
	\textbf{D}, as well as for \textbf{A} (the most popular incorrect 
	answer). 
	The student decides to keep the same answer choice \textbf{D}, and 
	indicates that the explanation $e_{25}$ is the most convincing.
	This is referred to as a \textit{Right}$\rightarrow$\textit{Right} 
	transition.
	(b) This observation is transformed into 8 explanation pairs. The first 
	pair is for the choice of $e_{25}$ over what the student wrote themselves, 
	and the other seven are for the choice of $e_{25}$ over the other shown 
	explanations. 
	The pairs are labelled as such that $e_{25}$ is the more convincing of the 
	pair. 
	(c) This pairwise preference data is aggregated global ranked list of 
	student explanations for this question, where each explanation is assigned 
	a real-valued rank score (using the methods described in section 
	\protect\ref{sec:rank_agg}).
}
\label{fig:make_pairs_a}
\end{figure}

Using these explanation pairs, we apply the following rank aggregation 
techniques in order to derive a real valued \textit{convincingness} rank score, 
as in figure \ref{fig:make_pairs_a}(c).

\begin{enumerate}
	
	\item \textbf{WinRate}, defined as the ratio of times an explanation is 
	chosen to the number of times it was shown.
	
	\item \textbf{BT} score, which is the argument ``quality'' parameter 
	estimated for each explanation, according to the \textit{Bradley-Terry} 
	model, where the probability of argument A being chosen over argument B is 
	given by 
	
	$$
	P(a>b) = 
	\frac{1}{1+e^{\beta_b-\beta_a}}
	$$
	where $\beta_i$ is the latent strength parameter of argument $i$. 
	
	We decompose each student-item observation into argument pairs, where the 
	chosen explanation is paired with each of the other shown ones, and the 
	pair is labelled with $y=-/+1$, depending on whether the chosen explanation 
	is first/second in the pair.
	Assuming there are $N$ explanations, labelled by $K$ students, and $S_K$ 
	labelled pairs,the latent strength parameters are estimated by maximizing 
	the log-likelihood given by:
	$$
	\ell(\boldsymbol{\beta})=\sum_{K}\sum_{(i,j)\epsilon S_K}^{} 
	\log\frac{1}{1+e^{\beta_i - \beta_j}}
	$$
	subject to $\sum_{i}\beta_i=0$.
	
	
	\item The \textbf{Elo} rating system \cite{elo_rating_1978}, which was 
	originally proposed for ranking chess players, has been successfully used 
	in adaptive learning environments (see \cite{pelanek_applications_2016} for 
	a review). 
	This rating method can be seen as a heuristic re-parametrization of the 
	\textbf{BT} method above, where the probability of argument A being chosen 
	over argument B is given by
	$$
	P(a>b) = P_{ab} = \frac{1}{1+10^{(\beta_b-\beta_a)/\delta}}
	$$
	where $\delta$ is a constant. 
	All arguments are initialized with an initial strength of $\beta_0$, and 
	the rating of any argument is only updated after it appears in a pairwise 
	comparison with another.
	The rating update rule transfers latent ``strength'' rating points from the 
	loser, to the 
	winner, in proportion to the difference in strength:
	
	$$
	\beta_a':=\beta_a+K(P_{ab} - \beta_a)
	$$
	
	While the \textbf{BT} model can be thought of a \textit{consensus} 
	approach (all rank scores are re-calculated after each pair is seen), 
	\textbf{Elo} ratings are dynamic and implicitly give more weight 
	to recent data \cite{aldous_elo_2017}.
	
	\item \textbf{Crowd-BT} \cite{chen_pairwise_2013} is an extension of the 
	\textbf{BT} model, tailored to settings where different annotators may have 
	assigned opposite labels to the same pairs, and the reliability of each 
	annotator may vary significantly. 
	A reliability parameter $\eta_k$ is estimated for each student, 
	
	$$
	\eta_k \equiv P(a >_k b | a >b )
	$$
	
	where $\eta_k \approx 1$ if the student  $k$ agrees with most other 
	students, and $\eta_k \approx 0$ if the student is in opposition to their 
	peers.
	This changes the model of argument $a$ being chosen over $b$ by student $k$ 
	to 
	$$
	P(a >_k b) = 
	\eta_k \frac{e^{\beta_a}}{e^{\beta_a}+e^{\beta_b}} + (1-\eta_k) 
	\frac{e^{\beta_b}}{e^{\beta_a}+e^{\beta_b}}
	$$
	and the log-likelilood maximized for estimation to 
	
	$$
	\ell(\boldsymbol{\eta},\boldsymbol{\beta})=\sum_{K}\sum_{(i,j)\epsilon 
		S_K}^{} 
	\log \left[ \eta_k \frac{e^{\beta_a}}{e^{\beta_a}+e^{\beta_b}} + (1-\eta_k) 
	\frac{e^{\beta_b}}{e^{\beta_a}+e^{\beta_b}} \right]
	$$
	
\end{enumerate}

How we evaluate the fit of these rank aggregation methods to our data is 
described in section \ref{sec:methodology_eval}

\subsection{Modelling rank scores}\label{sec:features}
We build on the results from the previous section to now predict these 
aggregate scores for each explanation, using linguistic properties of those 
explanations.
We address \textbf{RQ2} with a regression task of predicting the argument 
\textit{convincingness} scores via a feature-rich document vector.

Recent experimental results posted state-of-the-art results for this same 
regression task on a large argument mining dataset, using a neural embeddings 
in a bidirectional encoder representations from transformers (BERT) 
\cite{gretz_large-scale_2019}.
However we favour a feature-rich approach and simpler learning algorithms, 
keeping in mind downstream priorities such as interpretability for teachers in 
their reporting tools.

The list of features included here are derived from related work in argument 
mining \cite{habernal_which_2016}\cite{persing_end--end_2016} on student 
essays, automatic short answer scoring \cite{mohler_text--text_2009}.

\begin{itemize}
	
	\item Surface Features: 
	sentence count, 
	max/mean word length, 
	max/mean sentence length;
	
	\item Lexical: 
	uni-grams, 
	type-token ratio, 
	number of keywords (defined by open-source discipline specific 
	text-book), 
	number of equations (captured by a regular expression);
	
	\item Syntactic: 
	POS n-grams (e.g. \textit{nouns, prepositions, 
		verbs,conjunctions,negation, adjectives, adverbs, punctuation}), 
	modal verbs (e.g. \textit{must, should, can, might}),
%	contextuality/formality measure \cite{heylighen_variation_2002},
	dependency tree depth;
	
	\item Semantic:
	\begin{itemize}
		\item Using pre-trained GloVe \cite{pennington_glove:_2014} vectors, we 
		calculate similarity metrics to i) all other explanations, ii) the 
		question item text, and, when available, iii) a teacher provided 
		``expert'' explanation.
		\item we derive our own discipline specific embedding vectors, trained 
		on corresponding open-source textbooks \footnote{https://openstax.org}. 
		We experiment with a word-based vector space model, Latent Semantic 
		Indexing (\verb|LSI|) \cite{deerwester_indexing_1990}, due to its 
		prevalence in text analytics in educational data mining literature, as 
		well as \verb|Doc2Vec| \cite{le_distributed_2014}, which directly 
		models the compositionality of all the words in a sentence 
		\footnote{model implementations from 
		https://radimrehurek.com/gensim/index.html}.
		We take the text of the question prompt, as well as an ``expert 
		explanation'' provided by teachers for each question, and determine the 
		10 most relevant sub-sections of the textbook.
		For each student explanation, we then calculate the mininum, maximum, 
		and mean cosine similarity to these 10 discipline specific ``reference 
		texts''.
		 
	\end{itemize}
		
%	\item Co-reference \cite{persing_end--end_2016}: 
%	fraction of entities from the prompt mentioned in each sentence, 
%	averaged over all sentences (using neural Co-reference resolution)
%	vector cosine similarity between student explanation and prompt, 
%	and answer choices; 
	
	\item Readability:
	Fleish-Kincaid reading ease and grade level,
	Coleman-Liau,
	automated readability index, 
	spelling errors
	
\end{itemize}

Features typical to NLP analyses in the context writing analytics that are not 
included here are cohesion, sentiment, and psycho-linguistic features, as they 
do not seem pertinent for shorter responses that deal with STEM disciplines.

 
\subsection{Evaluation of methodology}\label{sec:methodology_eval}

In order to evaluate our choice of rank aggregation method, and address our 
research question RQ1, we perform several validation tests.

The reference argument mining datasets that we use for this study, along 
with annotated pairwise preference data, each include their own derived 
aggregated rank score for each argument (described in 
\ref{sec:related_work:arg_quality}).
We begin our evaluation of the soundness of our choice of simpler rank 
aggregation methods, by measuring the correlation between our ranking scores, 
and the reference scores, on the AM datasets.
For each topic in the different AM datasets, we calculate the Pearson 
correlation between the ``reference'' score of each argument, and the simpler 
scores we choose to include in our methodology (\textit{WinRate},\textit{BT}, 
\textit{Elo}. We cannot include \textit{CrowdBT} here, as the AM datasets do 
not include an identifier for ``annotator'').
The distribution of Pearson correlation coefficients across the different 
topics are shown in the box plots in figure \ref{fig:corr_to_reference_score}.

\begin{figure}[H]
	\centering
	\scalebox{0.5}{\input{img/corr_to_reference_score.pgf}}
	\caption{Distribution of Pearson correlation coefficients measured between 
		``reference'' rank scores, and the rank aggregation methods (WinRate, 
		BT, 
		Elo) used in our proposed methodology, across the different topics of 
		the 
		reference argument mining datasets.}
	\label{fig:corr_to_reference_score}
\end{figure}

While the variance across topics of the correlation coefficients between the 
``out-of-the-box'' reference scores and our simpler rank-aggregation scores is 
quite large, the median lies between 0.5 and 0.7 for the \textbf{UKP} and 
\textbf{IBM\_ArgQ} datasets.
These are significantly higher than for \textbf{IBM\_Evi}, likely because the 
reference scores for this set are dependant on a specific Bi-LSTM architecture.
The relative alignment between our chosen rank aggregation techniques 
(\textit{WinRate}, \textit{Bradley-Terry}, and \textit{Elo}), and the modified 
PageRank score provided with \textbf{UKP}, indicates that all capture 
approximately the same information about overall \textit{convincingness}.
Also of note is the correlation between the \textbf{IBM\_ArgQ} reference rank 
score, and the methods we include in our methodology. 
The reference score here was actively collected by the authors of dataset, 
first by presenting crowd workers with individual arguments, and prompting them 
to give a binary score of 1/0, based on whether ``they found the passage 
suitable for use in a debate'', and then averaging the score over all labellers.
The correlation between \textit{WinRate}, \textit{Bradley-Terry}, and 
\textit{Elo}, and this actively collected reference score, would indicate that 
these methods capture a ``true'' ranked list.
   
In order to evaluate a measure of \textit{reliability} of these rankings, we 
employ a validation scheme similar to one proposed by \cite{jones_peer_2015}.
Students are randomly split into two batches, and their answers are used to 
derive two independent sets of rank scores, as shown in figure 
\ref{fig:evaluate_rankings}. 


\begin{figure}[H]
	\centering
	\scalebox{0.5}{\input{img/evaluate_rankings.pdf_tex}}
	\caption{
		Evaluating of \textit{reliability} of rank scores: for each question, 
		student answers are divided into two batches, yielding two batches of 
		corresponding pairs, and two aggregated rankings.
		Two measures of reliability of the derived rankings are shown with the 
		yellow arrows: 
		i) the rank scores of each batch of students can be used to predict the 
		pairwise preferences of the other batch, and 
		ii) the Kendall tau correlation coefficient can be calculated between 
		the two independently derived ranked lists for each batch of students. 
	}
	\label{fig:evaluate_rankings}
\end{figure}

We apply these evaluations of reliability on the derived rank scores from the 
pairwise preference data from \verb|dalite|, and dis-aggregate the results by 
possible TMPI transition types (figures \ref{fig:acc_by_batch} and 
\ref{fig:corr_by_batch}.

\begin{figure}[H]
	\centering
	\scalebox{0.6}{\input{img/acc_by_transition.pgf}}
	\caption{
		Comparing the average pairwise classification accuracy of different 
		rank aggregation scores in predicting which argument is more convincing 
		from a pair. 
		Rank scores are calculated with the vote data of half the students, and 
		tested on the pairs generated by the other half. 
		Data is averaged across all questions, dis-aggregated by different TMPI 
		transition types. 
	}
	\label{fig:acc_by_batch}
\end{figure}

It should be noted that, as shown in the relative proportions of the Sankey 
diagram (figure \ref{fig:tmpi_sankey}, the vast majority of the data is 
represented in the Right$\rightarrow$Right transition (the rarest transition is 
Right$\rightarrow$Wrong).
When we consider using the rankings derived from one batch of students, and use 
them to predict the pairwise preferences of the other batch, the classification 
accuracies are roughly equivalent across the different rank score methods 
(figure \ref{fig:acc_by_batch}).
All of the methods outperform a baseline ``Length'' method, which is where the 
pairwise preference is chosen by simply choosing the explanation with the most 
words.

However there seems to be a slight advantage with the \textit{Elo} method in 
evaluating the reliability of the rankings across independent batches of 
students if we consider the alignment between the rankings themselves (figure 
\ref{fig:corr_by_batch}).


\begin{figure}[H]
	\centering
	\scalebox{0.6}{\input{img/corr_by_batch.pgf}}
	\caption{
		Mean of Pearson correlation coefficients between independent rank 
		scores, derived from two independent batches of students, averaged over 
		all questions, dis-aggregated by different TMPI transition types.
	}
	\label{fig:corr_by_batch}
\end{figure}

In practice, after choosing the most reliable rank-aggregation scoring method, 
the second step of our proposed methodology is to address our second research 
question, \textbf{RQ2}, and build feature-rich supervised regression models to 
predict the individual argument scores.
We choose our feature sets based on relevant related research, as described in 
section \ref{sec:features}. 

In order to estimate the generalizability of these models to new question 
items, we employ a ``cross-topic'' cross-validation scheme, wherein we hold out 
all of the answers on one question item as the test set, training models on all 
of the answers for all other question items in the same discipline.
This approach is meant to capture discipline specific linguistic patterns, 
while addressing a the ``cold-start'' problem for new items before vote data 
can be collected.

As has been described in related work, argument \textit{length} is a difficult 
baseline to beat when modelling argument \textit{quality} in pariwise 
preference data.
The greater the amount of words, the greater the opportunity to construct a 
convincing argument.
The only way to counter this would be to have explanation pairs in the dataset 
where the same logic is present, but more concisely in one argument than in the 
other.
Since we cannot guarantee the presence of such data, we control for explanation 
length when training our feature-rich models. 
Namely, for each question/topic, we bin the student explanations by quartile 
for word-count, and then in our cross-topic validation scheme, we train and 
test models on corresponding quartiles.

Finally the last piece of our methodology is based on practical considerations:
more important than the real-valued \textit{convincingness} scores of all 
student explanations, is the ability for teachers to be able to quickly 
retrieve the top-K, or bottom-K ranked explanations for a particular question.
Thus we borrow from information retrieval research, and calculate a variant of 
\textit{precision@K}; e.g. how many of the top-5 ranked explanations (in 
the top quartile of longest explanations) does our feature-rich regressor 
capture in its own predicted top-5?


\section{Results \& Discussion}\label{sec:model_results}

One of the contributions of this study is to propose a methodology for the 
analysis and leveraging of learnersourced explanation quality labels inside 
TMPI learning environments, or more broadly speaking, any setting where there 
is an ordinal/comparative peer grading task for natural language student 
submissions.

One part of this methodological contribution is to frame the analysis as an 
information retrieval task: given a set of TMPI observations for students who 
each select a peer's explanation as more convincing than their own, taking into 
account the subset of explanations they are presented with, can we 
predict, based on linguistic properties of the explanations alone, which ones 
will be in the top-K most \textit{convincing}?

Figure \ref{fig:prec_at_K} gives an overview of our modelling results on the 
TMPI data collected from myDALITE.org using the methodology we describe above.
Each dot represents a question/topic from one of the reference AM datasets, or 
from a discipline from our TMPI data.
We plot the \textit{precision @ K} (where $K={1,3,5}$) of our feature-rich 
regression model, as a function of the explanations/arguments in the held-out 
test set.
(
The TMPI data all have a minimum of $N=20$ answers, due to the filtering 
criteria we describe in section \ref{sec:dataset_dalite}, and the 
disaagregation by word-count quartile.
We do not apply same filters on the AM datasets, which explains why they are 
all at the left of our plots.
)

\begin{figure}[H]
	\scalebox{0.5}{\input{img/prec_at_K.pgf}}
	\caption{
		Evaluating of regression models tasked with predicting 
		\textit{convincingness} score of arguments/TMPI-explanations based on 
		linguistic features.
		Evaluation metric is \textbf{precision @ K}, where we verify how many 
		of the predicted top-K ranked explanations are in the measured top-K 
		list (using ``winrate'' as a \textit{measure} of convincingness). 
		Precision is plotted against the size of the test-set on the horizontal 
		axis, under a cross-topic validation scheme.
		The dashed line is an approximation of the probability of 
		choosing the top-K explanations, purely by chance ($K/N$).
		Each dot represents the performance on one held-out 
		topic/TMPI-question-prompt, color-coded based on which 
		dataset/discipline it originates from.
		We compare Linear Regression with a Decision Tree Regressor in the two 
		columns.
	}
	\label{fig:prec_at_K}
\end{figure}

An estimate of the baseline probability that a model would correctly choose the 
top-$K$ explanations correctly out of $N$ items in the test set, at random, is 
given by $n\choose r$=$\frac{n!}{r!(n-r)!}$. 
(We plot a conservative estimate of this baseline in figure \ref{fig:prec_at_K},
$K/N$).

What we see is that \textit{precision @ K} for $K={1,3}$ remains a difficult 
task, given the feature set we have started with, as the precision remains 
below a random baseline for a larger relative proportion of question/topics.


\begin{table}
	\input{data/ranking_y_reference_pearson}
\end{table}


\begin{table}
	\input{data/ranking_y_reference_spearman}
\end{table}



\begin{table}
	\input{data/ranking_y_winrate_pearson}
\end{table}



\begin{table}
	\input{data/ranking_y_winrate_spearman}
\end{table}


\begin{table}
	\input{data/ranking_dalite_y_winrate_spearman}
\end{table}


\begin{table}
	\input{data/ranking_dalite_y_winrate_spearman}
\end{table}



\section{Acknowledgements}
Funding for the development of myDALITE.org is made possible by 
\textit{Entente-Canada-Quebec}, and the \textit{Ministre de l'ducation et 
Enseignment Suprieure du Qubec}. Funding for this research was made possible 
by the support of the Canadian Social Sciences and Humanities Research Council 
\textit{Insight} Grant. This project would not have been possible without the 
SALTISE/S4 network of researcher practitioners, and the students using 
myDALITE.org who consented to share their learning traces with the research 
community.

 

% REMOVE NOCITE OR IT WILL LIST EVERYTHING IN YOUR DATABASE AS A REFERENCE
%\nocite{*}

\bibliographystyle{acmtrans}
\bibliography{MyLibrary}

\end{document}

%%%%%%%%%%%%%%%%%%%
% Archive

%The goal \textbf{RQ1} is establish which rank aggregation methods are best 
%suited for the context of TMPI, such that one can take the comparative 
%preference data from many students who each see different subsets of peer 
%explanations.

%We experiment with vector space models with different document representations:
%\begin{enumerate}
%	\item LSA vectors (10,50,100 components) \cite{deerwester_indexing_1990}
%	\item Glove embeddings \cite{pennington_glove:_2014}
%	\item BERT embeddings \cite{devlin_bert_2018}, out-of-the-box, and 
%	fine-tuned for the current classification task
%\end{enumerate}

%\begin{figure}
%	\scalebox{0.6}{\input{img/corr_plot.pgf}}
%	\caption{
%		Correlation between different Ranking Scores for each explanation,  
%		disaggregated by transition type  
%	}
%	\label{fig:acc_by_rank_score_type}
%\end{figure}

%employ a time-series based cross-validation scheme:
%at each timestep, we calculate the aggregated argument \textit{convincingness} 
%scores from past students, and set out to predict: which arguments will be 
%chosen as more convincing from the pairs constructed for the current student?  
%\begin{figure}
%	\centering
%	\def\svgscale{0.5}
%	\input{img/make_pairs_time_series.pdf_tex}
%	\caption{
%		All of the pairs constructed for answers by students $i<=30$ are used 
%		to learn rank scores $\sigma$, which are evaluated in their ability to 
%		predict the label in the held out test set: argument pairs constructed 
%		from the answer of student $i=31$.
%		We must exclude those pairs which include the 31st student, as this 
%		explanation is unseen in the training set, and does not have a rank 
%		score 
%	}
%	\label{fig:make_pairs_time_series}
%\end{figure}


%\begin{figure}[H]
%	\centering
%	\def\svgscale{0.5}
%	\input{img/rankingreliability.pdf_tex}
%	\caption{
%		Validation scheme for evaluating reliability of rankings.
%		At each time step $\tau$ we take all students $i<=\tau$, and split 
%		student into two batches (chosen at alternating time steps)
%		We learn rankings for each of these batches, and evaluate the Kendall 
%		Tau rank correlation as estimate of reliability of the rankings.
%	}
%	\label{fig:rank_reliability}
%\end{figure}

%\begin{figure}
%	\scalebox{0.6}{\input{img/acc_by_rank_score_type.pgf}}
%	\caption{
%		Comparing the classification accuracy of different rank aggregation 
%		scores in predicting which argument is more convincing from a pair, 
%		under a time-series cross-validation scheme. Data is averaged across 
%		all questions and times steps, for different disciplinary subsets of 
%		TMPI data. 
%	}
%	\label{fig:acc_by_rank_score_type}
%\end{figure}


%\begin{figure}
%	\includegraphics[width=\linewidth]{img/acc_vs_time_Physics_ada.png}
%	\caption{to do}
%	\label{fig:acc_vs_time}
%\end{figure}
