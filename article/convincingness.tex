% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{pgf}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{fancyvrb}
\usepackage{todonotes}
\VerbatimFootnotes




\begin{document}
%
\title{Automatic Explanation Quality Assessment in Online Learning 
Environments: new datasets and methods}
\titlerunning{Convincingness \& Peer Instruction}
%
\author{Sameer Bhatnagar\inst{1} \and
Amal Zouaq\inst{1} \and
Michel C. Desmarais\inst{1} \and
Elizabeth Charles\inst{2}
}
%
\authorrunning{S. Bhatnagar et al.}

\institute{Ecole Polytechnique Montreal 
\email{\{sameer.bhatnagar,amal.zouaq,michel.desmarais\}@polymtl.ca}\and
Dawson College
\email{echarles@dawsoncollege.qc.ca}\\
}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
\textit{Asynchronous Peer Instruction} is increasingly popular in 
online learning environments. It relies on the principle of 
leveraging content that is both \textit{generated} and \textit{evaluated} by 
novices to foster learning and self-reflection. 
Students first respond to a question item, but 
they must also provide an explanation for their reasoning. 
They are then presented alternative explanations as written by their peers, and 
given the opportunity to change their initial answer choice, based on those 
they find most convincing. 
The peer-explanations that students find most convincing represent valuable 
data, for teachers to better grasp their students' understanding, and for the 
learning environment itself, as higher quality explanations can be shown to 
students as examples to compare their work to. 
This study reports on the application of argument mining methods in the 
context of asynchronous peer instruction, with the objective of automatically 
identifying high quality student explanations. 
Our results offer the potential to inform the design of ``learnersourcing'' 
systems, such as asynchronous peer instruction.
These design choices are critical as these systems scale, especially with 
respect to providing pedagogically insightful reports to teachers, and 
presenting engaging alternative explanations to students to promote higher 
order thinking. 

\keywords{Argument mining  \and Learnersourcing \and Peer Instruction}
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\todo[inline]{Must be anonymized before submission}
\section{Introduction}
As online learning environments scale in the number of question items, providing 
immediate and tailored feedback to students becomes intractable for activities 
that prompt for open ended responses. \textit{Peer assessment} and \textit{peer 
feedback} can address this issue, and have the added benefit of being an 
effective way to learn for the student who give the feedback as 
well~\cite{jhangiani_impact_2016}. 
However the drawback of such approaches often lie in the varying ability of 
novices to provide good feedback to their peers.
Frameworks such as Adaptive Comparative Judgement~\cite{pollitt_method_2012}, 
where teachers assess student submissions by simply choosing which is better 
from a pair, have been shown to be a reliable and valid alternative to absolute 
grading.
Thus it is not surprising that there are a growing number of online learning 
environments that extend this paradigm to students with \textit{comparative 
peer assessment}, wherein, after students submit their own work, they are asked 
to compare and contrast a pair of two of their peers' submissions, and provide 
feedback
\cite{univeristy_of_british_columbia_ubc/ubcpi_2019}
\cite{cambre_juxtapeer:_2018}
\cite{potter_compair:_2017}.

A subset of these tools then use the comparative peer assessment data to 
inform the choice of how the pairs are constructed, and which students are 
assigned to which pairs,
\cite{khosravi_ripple_2019}
\cite{williams_axis:_2016}
\cite{saltise_saltises4/dalite-ng_2019}.
This leads to the classic trade-off from the field reinforcement learning: 
exploiting the student submissions for which we have reliable data and can 
estimate their quality, while exploring student work that is newer to the 
database, and needs to be shown and evaluated in order to get an estimate of 
its quality.

Automatic assessment of quality is especially crucial in systems where the 
content generated by students is actually a central part of the pedagogical 
script: in platforms where students generate explanations, hints, or even new 
question items, which are then shown to future students, selecting bad content 
can negatively impact the learning of future students. 
We focus our attention on learning environments that enable \textit{peer 
instruction}\cite{crouch_peer_2001}, which are built on a two-stage script: (i) 
students are prompted to answer a multiple choice question, and provide a 
free-text explanation that justifies their answer; (ii) without revealing the 
correct answer, students are prompted to reconsider their answer, by presenting 
them a selection of explanations written by previous 
students~\cite{charles-woods_designing_nodate}.
Students can either decide that their own explanation is best, or indicate 
which of their peers' explanation was the most convincing.
In this instance of 
\textit{learnersourcing}\cite{weir_learnersourcing_2015}, this ``vote data'' 
is valuable at two levels: it can then be used to determine what to present to 
future students, but also inform instructors of their students' understanding 
of the material.

We frame the explanations to multiple choice questions as \textit{arguments} 
meant to persuade one's peers. As such, we explore methods and datasets from 
the \textit{argument mining} research community, where there has been a growing 
body of work on automatically assessing argument quality, along the dimension 
of \textit{convincingness}.
The objective of this study is to determine whether we can automatically assess 
the \textit{convincingness} of student explanations in peer-instruction based 
learning environments, starting from pairwise preference data generated by 
students in undergraduate level science courses. We apply our methods, and 
compare our results, across similar publicly available datasets of argument 
pairs, annotated for pairwise preference. We report on the effectiveness of 
vector space models, as well as state-of-the-art neural approaches, applied to 
the task of learning pairwise preferences for convincing arguments. Our 
findings suggest that the arguments generated in learning environments centred 
on undergraduate science topics present a more challenging variant of the task 
originally proposed in the argument mining community, and that classical 
approaches match neural models for performance on this task across datasets.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}


\subsection{Learnersourcing \& Comparative Peer Assessment}
The term ``learnersourcing'' was coined by \cite{weir_learnersourcing_2015}, 
and defined as the process in ``which learners collectively generate useful 
content for future learners while engaging in a meaningful learning experience 
themselves.'' 
One of the earliest examples of a learning environment centred on this mode is  
Peerwise~\cite{denny_peerwise:_2008}, wherein students generate question items 
for the subjects they are learning, share them with their peers, so that others 
can use them for practice. 
Ripple~\cite{khosravi_ripple_2019} is a similarly built system, but they add a 
recommendation engine which adaptively selects which problems to suggest to 
which students.

Other tools leave the creation of question items to teachers, but call on 
students to generate and evaluate \textit{explanations} for the answers.
The AXIS~\cite{williams_axis:_2016} system prompts students to provide an 
generate explanation for their response to a short answer question, and then 
evaluate a similar explanation from one of their peers on a scale of 1-10 for 
\textit{helpfulness}. This rating-data drives the reinforcement learning 
algorithm that then decides which explanations to show to which future students.

A similar class of learning platforms leverage comparative judgement for 
supporting how students can evaluate the work of their peers. 
Juxtapeer~\cite{cambre_juxtapeer:_2018} asks students to provide feedback to a 
single peer on their work by explicitly comparing to that of another.
ComPAIR~\cite{potter_compair:_2017} asks students for feedback on each item of 
the pair they are presented with, with a focus on what makes one ``better'' 
than the other.

Finally, \textit{peer instruction} question prompts are being used more 
frequently inside online learning 
assignments~UBCPI~\cite{univeristy_of_british_columbia_ubc/ubcpi_2019}
\cite{charles_harnessing_2019}.
The student is presented with a multiple choice item, and prompted to provide 
an explanation of their answer choice. 
In order to encourage students to reflect on their own reasoning, they are then 
presented with a subset of alternative peer explanations to their own answer 
choice, as well for another choice. The student is then given the chance to 
revise their answer choice (or not), by indicating which of the peer 
explanations was most \textit{convincing} from the subset.


\subsection{Argument Quality \& Convincingness}
Conventional argument-mining pipelines include several successive components, 
starting with the automatic detection of argumentative units, classification of 
these units into types (e.g. major claim, minor claim, premise), and 
identification of argumentative relations (which evidence units support which 
claim). 
Such pipelines are essential in question-answering systems 
\cite{lippi_argumentation_2016} and are at the heart if the IBM Project Debater 
initiative. 

Work in the area of automatic evaluation of argument quality finds its roots in 
detecting evidence in legal texts~\cite{moens_automatic_2007}, but has 
accelerated in recent years as more datasets become available in everyday 
contexts, and focus shifts to modelling more qualitative measures, such as 
\textit{convincingness}. 

Some of the earlier efforts included work on automatically 
scoring of persuasive essays \cite{persing_end--end_2016} and modelling 
persuasiveness in online debate forums \cite{tan_winning_2016}. 
However, evaluating argument \textit{convincingness} with an absolute score can 
be challenging, which has led to significant work in adopting a pairwise 
approach, where data consists of pairwise observations of two arguments, 
labelled with which of the two is most convincing.

In \cite{habernal_which_2016}, the authors propose a feature-rich support 
vector machine, as well as an end-to-end neural approach based on pre-trained 
Glove vectors and a bidirectional Long-Short-Term Memory network for the 
pairwise classification task. 
This is extended in \cite{gleize_are_2019}, where the authors build a Siamese 
network architecture, where each leg is a BiLSTM, taking as input the pair of 
explanations as Glove embeddings \cite{pennington_glove:_2014}, in order to 
detect which of argument in a pair 
has the most convincing evidence.
Finally, based on the success of transformer models such as 
BERT~\cite{devlin_bert_2018}, the authors of \cite{toledo_automatic_2019}  
release a dataset of argument pairs and show that these models accurately 
predict the most convincing argument in a pair.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
\section{Data}

One of the objectives of this study is to compare and contrast how argument 
mining methods for evaluating argument quality, specifically for argument 
\textit{convincingness}, perform in an online learning environment with 
learner-generated and annotated arguments. Along with myDALITE data, we include 
in our study two publicly 
available datasets, each specifically curated for the task of automatic 
assessment of argument quality along the dimension \textit{convincingness}. 
Table \ref{tab:data_summary} summarizes some of the descriptive statistics that 
can be used to compare these sources, and potentially explain some of our 
experimental results.

\begin{table}
	\begin{subtable}[t]{\textwidth}
	\input{publication_artefacts/data/sample_obs_UKP}
	\end{subtable}
	\begin{subtable}[t]{\textwidth}
	\input{publication_artefacts/data/sample_obs_IBM}
	\end{subtable}

	\caption{Examples of argument pairs from reference datasets}
	\label{tab:sample_obs}
\end{table}



\subsection{UKP \& IBM}
\textbf{UKPConvArgStrict}\cite{habernal_which_2016}, hence forth referred to as 
\textbf{UKP}, was the first to propose the task of pairwise preference learning 
for argument convincingness. 
The dataset consists of just over 1k individual arguments, that support a 
particular stance for one of 16 topics. 
These arguments were distributed as 11.6k pairs to annotators on a 
crowd-sourcing platform, where the task was to choose which of the two 
arguments, for the same stance regarding the same topic, was more convincing. 

More recently, a second similar dataset, 
\textbf{IBMArgQ-9.1kPairs}\cite{toledo_automatic_2019}, henceforth 
referred to as \textbf{IBM\_ArgQ}, which is made of 3.4k individual arguments 
for 11 topics, assembled into 9.1k pairs labelled for which is more convincing. 
One of the key differences between these two is that \textbf{IBM\_ArgQ} data is 
more strongly curated with respect to the relative length of the arguments in 
each pair: in order to control for the possibility that annotators may make 
their choice of which argument in the pair is more \textit{convincing} based 
merely on the length of the text, the mean difference in word count, 
$\overline{\Delta wc}$, is just 3 words across the entire dataset, which is 10 
times more homogeneous than pairs in \textbf{UKP}.

Finally, we include a fourth dataset, \textbf{IBM\_Evi}, consisting of 1.5k 
individual arguments, organized into 5.2k pairs annotated for pairwise 
preference for convincingness~\cite{gleize_are_2019}. The important distinction 
here is that the arguments are actually extracted as evidence for their 
respective topic from Wikipedia, and hence represent cleaner well-formed text 
than our other reference datasets.


\begin{table}
	\caption{Descriptive statistic for each dataset of argument pairs, with 
		last rows showing \textit{dalite} split by discipline.}
	\centerline{\input{publication_artefacts/data/df_summary_final}}
	\label{tab:data_summary}
\end{table}

\subsection{myDALITE}
The dataset is comprised of pairs of student explanations for a particular 
answer choice to a given question. 
The first explanation is always the one written by the learner-annotator, while 
the second is an alternative which they either chose as more convincing, or 
not. 
In this respect, our peer-instruction dataset is different than the one 
mentioned above, since its is the same person, the student, who wrote the 
explanation, and as well as evaluated the its relative convincingness in the 
pair.

To ensure internal reliability, we only keep explanations that were chosen at 
least \input{publication_artefacts/data/VOTES_MIN}times. 
To ensure that the explanations in each pair are of comparable length, we keep 
only those with word counts that are within 
\input{publication_artefacts/data/MAX_WORD_COUNT_DIFF}words of each other. 

\begin{table}
	\input{publication_artefacts/data/sample_obs_dalite}
	\caption{Example of argument pair constructed from dalite dataset}
\end{table}

This leaves us a dataset with \input{publication_artefacts/data/N}observations, 
spanning \input{publication_artefacts/data/n_students} learner annotators having 
completed, on average, 
\input{publication_artefacts/data/avg_q_per_student} items each, from a total of 
\input{publication_artefacts/data/n_questions} items across three disciplines, 
with at least 
\input{publication_artefacts/data/MIN_RECORDS_PER_QUESTION}explanation-pairs 
per item.


\begin{table}
	\caption{Observations of students choosing a peer explanation as 
		more 
		convincing than their own, or not, aggregated by discipline and whether 
		they 
		started and finished with the correct answer.}
	
	\centerline{\input{publication_artefacts/data/transitions_by_discipline}}
	
	\label{tab:transitions_by_discipline}
\end{table}

Table \ref{tab:transitions_by_discipline} highlights one key difference between 
the modelling task of this study, and related work in argument mining, where 
annotators are presented pairs of arguments that are always for the same 
stance, in order to limit bias due to their opinion on the motion when 
evaluating which argument is more convincing.
In a \textit{Peer Instruction} learning environment, other pairings are 
possible and pedagogically relevant. 
In this dataset, the majority of students keep the same answer choice between 
the two steps of the prompt, and so they are comparing two explanations that 
are either both correct (\textit{``rr''}) or incorrect (\textit{``wr''}). 
However, there is \input{publication_artefacts/data/frac_switch}\% of the 
observations in this dataset are for students who not only choose an 
explanation more convincing than their own, but also switch answer choice, 
either from the incorrect to correct, or the reverse. 
These pairs add a different level of complexity to the model, but are very 
pertinent in the pedagogical context: what are the argumentative features which 
can help students remediate an initial wrong answer choice (\textit{``wr''})?
What are the features that might be responsible for getting students to 
actually move away from the correct answer choice (\textit{``rw''})?



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methods}

%\todo[inline]{This section might be before the data sets and should clearly 
%name and describe the methods.....@Michel: with the new table describing State 
%of the Art performance of these methods on different datasets, i wonder if now 
%it is better to have methods after data?}
Choosing which argument is more convincing from a pair, is a binary 
ordinal regression task, where the objective is to learn a function that, given 
two feature vectors, can assign the better argument a rank of $+1$, and the 
other a rank of $-1$.   
It has been proven that such a binary ordinal regression problem, can be cast 
into an equivalent binary classification problem, wherein the model is trained 
on the \textit{difference} of the feature vectors of each argument in the pair 
\cite{herbrich_support_1999}. 
Referred to as \textit{SVM-rank}, this method of learning pairwise preferences 
has been used extensively in the context of information retrieval (e.g. ranking 
search results for a query based on past clickthrough data) 
\cite{joachims_optimizing_2002}, but also more recently in evaluating the 
journalistic quality of newspaper and magazine articles \cite{louis_what_2013}, 
and for predicting which argument is more convincing~\cite{habernal_which_2016}.

\subsection{Vector Space Models}

We follow-up on this work, building simple vector space models to represent our 
text. 
We take all of the individual arguments for a particular topic in our training 
set (known as  ``explanations'' for a particular question item in the case of 
the \textbf{dalite} data), lemmatize the tokens, and build term-document 
matrices for each topic.
We then take the arithmetic difference of these normalized term frequency 
vector representations of each argument to train Support Vector Machine 
classifiers on. We refer to this model as \textbf{ArgBow}.


\subsection{Pre-trained word embeddings}
A limitation of vectors space models is the exclusion of words that are 
``out-of-vocabulary'' between the test set and the training data.
This has led to experiments for this 
task\cite{habernal_which_2016}\cite{gleize_are_2019} with language models that 
offer pre-trained word-embeddings that have already learned from massive 
corpora of text.    
In our \textbf{ArgGlove} model, we encode each token of each argument using 
300-dimensional GloVe vectors~\cite{pennington_glove:_2014}, 
and represent each argument as the average of its token embedding vectors. 
We then feed these into the same SVM-rank architecture described above. 

\subsection{Transfer Learning}
Finally, in order to leverage recent advances in transfer-learning for NLP, the 
final model we explore is \textbf{ArgBert}.
We begin with a pre-trained language model for English built using  
Bi-directional Encoder Representation from Transformers, known as 
\textit{BERT}\cite{devlin_bert_2018}, trained on large bodies of text for the 
task of masked token prediction and sentence-pair inference. 
As proposed in \cite{toledo_automatic_2019}, we take the final 768-dimensional 
hidden state of the base-uncased BERT model, feed it into a binary 
classification layer, and fine-tune all of the pre-trained weights 
using our argument-pair data. 
As in other applications involving sentence pairs for BERT, each argument pair 
is encoded as \verb|[CLS] A [SEP] B]|, where the special \verb|[SEP]| token 
instructs the model as which token belong to argument \verb|A| and which to 
argument \verb|B|. 
\footnote{modified from the \verb|run_glue.py| 
script provided by the \verb|tranformers| package, built by company hugging 
face. All code for this study provided in associated github repository}.

\subsection{Cross-Validation \& State of the Art}

The objective of this study is to apply methodology from the argument mining 
research community to inform the design of environment built upon 
learnersourcing of student explanations, so as to be able to be present student 
explanations that are \textit{convincing} to students as they engage with their 
reasoning when they answer conceptual questions.
As a baseline, we build a baseline model, \textbf{ArgLength}, which is trained 
on simply the number of words in each argument, as there may be many contexts 
where students will simply choose the longer/shorter argument, based on the 
prompt. 

In order to get a reliable estimate of performance, we employ stratified 5-fold 
cross-validation for our experiments.
This means that for each fold, we train our model on 80\% of the available 
data, ensuring that each topic and output class is represented equally. 
In the context of peer instruction learning environments, this is meant to give 
a reasonable estimate of how our models would perform in predicting which 
explanations will be chosen as most convincing, \textit{after} a certain number 
of responses have been collected. 

In Table \ref{tab:sota}, we denote, to the best of our knowledge, the 
state-of-the-art performance for each dataset on the task of pairwise 
classification for \textit{convincingness}. These values are meant provide a 
reference as we present our results in the next section, but cannot be directly 
compared, as we employ a stratified 5-fold cross-validation scheme, while the 
reference studies measure cross-topic validation.

\begin{table} 
\centering\begin{tabular}{l|*{2}{c}r}
	Dataset     & Acc & AUC & model  	   \\
	\hline
	UKP 		& 0.83 & 0.89 & ArgBERT\cite{toledo_automatic_2019}    \\
	IBM\_ArgQ   & 0.80 & 0.86 & ArgBERT\cite{toledo_automatic_2019}    \\
	IBM\_Evi    & 0.73 & - 	  & EviConvNet\cite{gleize_are_2019} \\
\end{tabular}
\caption{State of the art performance for pairwise argument classification of 
convincingness for three publicly available datasets, using cross-topic 
validation scheme}
\label{tab:sota}
\end{table}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}

\begin{itemize}
	\item ArgLength sets baseline for each dataset. IBM datasets curate most 
	aggressively for pairs with comparable lengths.
	\item Vector space models performance comparable to BERT.
	\item GloVe most stable across datasets. UKP and IBMEvi performance on 
	ArgBoW due to wide-ranging vocabulary (lots of unknown tokens in test set), 
	which reveals limitation of approach 
\end{itemize}


\begin{figure}
	\begin{subfigure}[t]{0.5\linewidth}
		\centering
		\scalebox{0.5}{\input{publication_artefacts/img/acc.pgf}}
		\caption{Accuracy}
	\end{subfigure}%
	\qquad
	\begin{subfigure}[t]{0.5\linewidth}
		\centering
		\scalebox{0.5}{\input{publication_artefacts/img/AUC.pgf}}
		\caption{RoC-AuC}
	\end{subfigure}
		\caption{Pairwise ranking classification accuracy and ROC-AUC for 
		different models across datasets}
	\label{fig:performance}
\end{figure}


\begin{figure}
	\begin{subfigure}[t]{0.5\linewidth}
		\centering
		\scalebox{0.5}{\input{publication_artefacts/img/acc_dalite.pgf}}
		\caption{Accuracy}
	\end{subfigure}%
	\qquad
	\begin{subfigure}[t]{0.5\linewidth}
		\centering
		\scalebox{0.5}{\input{publication_artefacts/img/AUC_dalite.pgf}}
		\caption{RoC-AuC}
	\end{subfigure}	
		\caption{Pairwise ranking classification accuracy and ROC-AUC for 
		different models in myDalite dataset, across disciplines}
		\label{fig:performance_dalite}
\end{figure}

\begin{itemize}
	\item performance of in chemistry worst, physics best, given their relative 
	proportions in data
	\item proportionately more errors for observations switch their answer 
	(check if true in IBM Evi, where stances can be different)
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

In \cite{louis_what_2013}, for task of pairwise ranking of newspaper articles 
based on ``quality'', the authors achieve a similar result: when comparing the 
performance of SVM-rank models using different input feature sets (e.g. 
\textit{use of visual language}, \textit{use of named entities}, 
\textit{affective content}), their top performing models achieve ``same-topic'' 
pairwise ranking accuracy of 0.84 using a combination of content and writing 
features, but also a 0.82 accuracy with the content words as features alone.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future Work}
\cite{nguyen_computational_2015} and \cite{louis_what_2013} use a combination 
of writing and content (BoW) features to achieve their best results, and thus 
this avenue must be explored more thoroughly, especially as this maybe vary 
across disciplines an teaching contexts. 
The state-of-the-art performance for pairwise classification accuracy on the 
\textbf{IBM\_Evi} dataset employs Siamese architechture\cite{gleize_are_2019}, 
where each leg of the network  is a Bi-directional Long-Short-Term-Memory 
network, with documents represented by word2vec embeddings. These variations 
are left to explore in future work.

In this study we do not ever infer which are, overall, the most convincing 
student explanations for any given item. Inferring a gold standard of global 
rankings, starting from these pairwise preference data can be accomplished 
using research from the information retrieval 
community~\cite{chen_pairwise_2013}. Work on deriving point wise scores for 
argument pairs is proposed as a Gaussian Process Preference Learning task by 
\cite{simpson_finding_2018}. Seeing the lack of pointwise labels for overall 
convincingness, \cite{toledo_automatic_2019} released a dataset where they 
collect this data as well. A comparable source of data inside the myDALITE 
platform are the feedback scores teachers can optionally provide to students on 
their explanations.


 \bibliographystyle{splncs04}
 \bibliography{MyLibrary}
\end{document}

%%%%%%%%%%%%%%%%%%
% ARCHIVE
%
%\begin{table}
%\begin{tabular}{ |l|l|l|}
%	\hline
%	Feature Type & Feature & $\tau$ \\
%	\hline
%	\multirow{6}{*}{Lexical} 
%	& Uni+BiGrams & \\
%	& Spelling Errors &  \\
%	& Equations & \\
%	& Type Token Ratio &  \\
%	& Punctuation &\\
%	& Readability scores&\\
%	\hline
%	\multirow{5}{*}{Syntactic} 
%	& PoS Uni+Bigrams &   \\
%	& Dependancy Tree Depth  & \\
%	& Conjunctions  & \\ 
%	& Modal Verbs & \\ 
%	& Tree Production Rules &\\
%	\hline
%	\multirow{3}{*}{Semantic} 
%	& LSA Similarity &  \\
%	& LSA Similarity Question & \\
%	& Likelihood (Textbook) & \\ 
%	\hline
%\end{tabular}
%
%\caption{Features used in experiments with Linear SVM, annotated with 
%	kendall $\tau$ correlation and sigificance with target label}
%\label{tab:features}
%\end{table}




%\begin{figure}
%	\subfloat[Baseline models on myDalite]
%	{	
%	\input{data/results_overall_myDalite}
%    }\hfill
%	\subfloat[Baselines on IBM argument ranking dataset]
%	{	
%	\input{data/results_overall_IBMPairs}
%    }
%	\label{tab:baselines}
%\end{figure}

%\begin{figure}
%	\subfloat[BERT model, fine tuned on myDalite data, by discipline]
%	{	
%		\input{data/BERT_dalite}
%	}\hfill
%	\subfloat[BERT model performance on Biology data, by answer transition]
%	{	
%		\input{data/BERT_Bio_transition}
%	}
%	\label{tab:BERT_dalite}
%\end{figure}

%\begin{figure}
%	\subfloat[BERT model, fine tuned on myDalite data, by discipline]
%	{	
%		\input{data/BERT_dalite}
%	}\hfill
%	\subfloat[BERT model performance on Biology data, by answer transition]
%	{	
%		\input{data/BERT_Bio_transition}
%	}
%	\label{tab:BERT_dalite}
%\end{figure}
%For our experiments, we begin by following the line of work proposed by 
%\cite{habernal_which_2016}, and experiment with a feature-rich linear SVM 
%classifier for the pairwise classification task. We use a similar feature set, 
%which we categorize as \textbf{lexical}, \textbf{syntactic}, and 
%\textbf{semantic}, as described in Table \ref{tab:features}. we begin by 
%computing the feature vector for each explanation, and compute the difference 
%for each pairwise ranking instance as per the well established SVM-Rank 
%algorithms \cite{joachims_optimizing_2002}, training the model to learn which 
%of the pair is the more convincing argument. 
%

%In pairwise classification tasks so 50\% is the baseline performance.  
%In Table \ref{fig:baselines} we begin by comparing the \textit{ArgLongest} 
%model baseline, where we predict that students simply choose the longer 
%explanation of the pair.
%We also include two baseline models on \textit{Bag of Words} models: 
%\textit{ArgBoWGen} where the term-document-matrix is built from an open-source 
%textbook from the corresponding discipline\footnote{https://openstax.org/}, 
%and 
%\textit{ArgBoWItemSpec}, where the term-document matrix is built from the 
%words 
%students have used for the item (pertinent when no reference text is available 
%for a discipline).
%As this is a new context for these argument mining methods, we include the 
%same 
%baselines on the carefully curated \textit{IBMArgPair} dataset, which is of 
%the 
%same format.
