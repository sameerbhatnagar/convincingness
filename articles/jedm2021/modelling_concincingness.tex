
\documentclass[notitlepage,12pt]{jedm}
%\usepackage[sc,sf,small]{titlesec}
\usepackage[table]{xcolor}
\usepackage{url}
\usepackage{hyperref}
\usepackage{pgf}
\usepackage{newtxmath}
\usepackage{booktabs}

\usepackage{ifthen}
\usepackage[utf8]{inputenc}
\usepackage{float}
\usepackage{calc}
\usepackage{multirow}
\usepackage{graphicx,caption,subcaption}
\usepackage[finalnew]{trackchanges}


\hypersetup{
	colorlinks   = true, %Colours links instead of ugly boxes
	urlcolor     = blue, %Colour for external hyperlinks
	linkcolor    = blue, %Colour of internal links
	citecolor   = blue %Colour of citations
}

%-----------------------------------------------------------------------
% FINAL COPYEDITTED SUBMISSION - UNCOMMENT THIS TO SUPPRESS PAGE NUMBERS
%\pagenumbering{gobble}
%-----------------------------------------------------------------------

\begin{document}
	
	\title{Modelling Argument Quality in Technology Mediated Peer Instruction}
	\date{} %do not delete this, it suppresses insertion of the date
	
	\author{
		{\large Sameer Bhatnagar}
		\\Polytechnique Montreal
	 	\and 
	 	{\large Michel C. Desmarais}
	 	\\Polytechnique Montreal
	 	\and 
	 	{\large Amal Zouaq}
 		\\Polytechnique Montreal
 }

	
	\maketitle
	
	\begin{abstract}
		TO DO
		\\ %Keep \\ for spacing to keywords
		
		{\parindent0pt
			\textbf{Keywords:} 
		}
	\end{abstract}

\section{Introduction}
Technology-mediated peer instruction \textit{(TMPI)} platforms 
\cite{charles_harnessing_2019}\cite{univeristy_of_british_columbia_ubc/ubcpi_2019}
expand multiple choice items into a two step process.
On the first step, students must not only choose an answer choice, but also 
provide an explanation that justifies their reasoning, as shown in figure 
\ref{fig:question_start}.

On the second step (figure \ref{fig:question_review}), students are prompted to 
revise 
their answer choice, by taking into consideration a subset of explanations 
written by their peers.

The student now has three options:
\begin{enumerate}
	\item Change their answer choice, by indicating which of their peer's 
	explanations for a \textit{different} answer choice was most convincing;
	\item keep the \textit{same} answer choice, but indicate which the peer's 
	explanations the student found more convincing than their own;
	\item choose ``I stick to my own'', which indicates that they are keeping 
	to the same answer choice, and that their own explanation is best from 
	among those that are shown.
\end{enumerate}


Whenever the student goes with either of the first two scenarios above, we 
frame this as ``casting a vote'' for the chosen peer explanation.

The design and growing popularity of TMPI is inspired by three schools of 
thought: firstly, prompting students to explain their reasoning is beneficial 
to their learning \cite{chi_eliciting_1994}. 
Deliberate practice of argumentation in defence of one's ideas has been shown 
to improve informal reasoning for science students\cite{venville_impact_2010}.
There exists empirical evidence on the positive relationship between 
constructing formally sound arguments and deep cognitive elaboration, as well 
as individual acquisition of knowledge\cite{stegmann_collaborative_2012}.


\begin{figure}[H]
	\begin{subfigure}[b]{0.4\textwidth}
		\def\svgscale{0.50}
		\input{img/tmpi_question_start.pdf_tex}
		\caption{
			The first step in TMPI, where a student is presented with a 
			multiple choice item. The student must enter a ``rationale'', or 
			``explanation'' justifying their answer choice.
			\newline
			\newline
			The panel on the right, figure \protect\ref{fig:question_review}, 
			shows the second, review step of TMPI.
			Before any feedback is given on the correctness of their first 
			attempt, the student is prompted to reconsider their answer choice, 
			by reading a subset of explanations written by previous students.
			A set of peer-explanations is shown for the student's own answer 
			choice, and another set is shown for a different answer choice. 
		}
		\label{fig:question_start}
	\end{subfigure}
	\qquad
	\begin{subfigure}[b]{0.6\textwidth}
		\def\svgscale{0.50}
		\input{img/tmpi_question_review.pdf_tex}
		\caption{}
		\label{fig:question_review}
	\end{subfigure}
	\caption{The two steps in technology-mediated peer 
		instruction (TMPI)}
	\label{fig:tmpi}
\end{figure}


Second, classroom based \textit{Peer Instruction} \cite{crouch_peer_2001}, 
often mediated by automated response systems (e.g. clickers), has become a 
prevalent, and often effective component in the teaching practice of 
instructors looking to drive student engagement as part of an active learning 
experience \cite{charles_beyond_2015}. 
In discussing with peers \textit{after} they have formulated their own 
reasoning, students are engaged in a higher order thinking task from Bloom's 
taxonomy, as they evaluate what is the strongest argument, before answering 
again.
Thirdly, by capturing data on which explanations students find most convincing, 
TMPI affords teachers the opportunity to mitigate the ``expert blind spot'' 
\cite{nathan_expert_2001}, addressing student misconceptions they might not 
otherwise have thought of.

We situate student explanations from TMPI, in the context of computational 
argumentation, a sub-field of NLP focused on identifying argumentative 
components, and in their links to one another.
Modelling argument ``quality'' is an area of active research, with direct 
applications in education, such as in automated scoring of 
persuasive essays written by students \cite{persing_modeling_2015} 
\cite{nguyen_argument_2018}.
When students are coached asked to debate in dyads, and prompted to either find 
consensus, or to persuade peers, there is correlation to the quality of 
arguments students produce, as measured by the presence of formal argumentative 
structures (e.g. claims, premise, etc.) \cite{garcia-mila_effect_2013}.

However experiments have also shown that the perceived quality of an argument 
depends on the audience, and so we follow a more pragmatic measure of argument 
quality, centred on the premise that the goal of argumentation is 
persuasion \cite{mercier_why_2011}.
Therefore we suggest that the ``vote'' data collected for each student's 
explanation, is a proxy for argument quality, along the dimension of 
\textit{convincingness}, as judged by peer learners. 
This is a direct application of the argument mining task originally proposed by 
\cite{habernal_which_2016}: if crowd-workers are presented with a pair of 
arguments for the same stance of a debatable topic, can we predict which of the 
two they will choose as more convincing?
This task has been extended to TMPI, wherein the objective is to predict which 
explanations students will choose as more convincing than their own 
\cite{bhatnagar_learnersourcing_2020}.

These votes can be aggregated into a \textit{convincingness} score, as a 
measure of how effective that explanation is in persuading peers to change 
their own answer.
Student explanations can then be ranked along such a score, allowing for 
instructors to gain insights on the thinking of their students with respect to 
specific content, and potentially even help students to improve how they 
communicate ideas within their discipline.

The problem of aggregating the results of evaluative peer-judgments extends  
beyond TMPI.
For example, in response to the difficulty students can have providing a 
holistic score to their peers' work, there is a growing number of peer-review 
platforms built on \textit{comparative} judgments.
Notable examples include ComPAIR \cite{potter_compair:_2017} and 
JuxtaPeer \cite{cambre_juxtapeer:_2018}, both of which present students with a 
just a pair of their peers' submissions, and prompt the learner to evaluate 
them with respect to one another.
As in TMPI, students apply a comparative judgment to only the subset of peer 
content that they are shown during the review step.
There is a need for a principled approach to aggregating this learnersourced 
data, in a pedagogically relevant manner, despite the inevitable absence of 
some ``true'' ranking.

This sets the stage for our central research questions: 
\begin{itemize}
	\item[RQ1] since each student's ``vote'' in this context represents an 
	incomplete evaluative judgement, which rank aggregation 
	methods are best suited for ranking the quality of student 
	explanations in TMPI?
	\item[RQ2] once we establish a ranked list of explanations along the 
	dimension of \textit{convinciness}, can we model this construct, and 
	identify the linguistic features of the most effective student 
	explanations, as judged by their peers?
\end{itemize}

Work on modelling \textit{convincingness} has, in large part, been centred on 
web discourse data.
In the educational setting, previous work in automated scoring of persuasive 
essays has focused on modelling holistic scores given by \textit{experts} on 
longer form essays.
To our knowledge, we are among the first to aggregate and model student 
``votes'', in order to evaluate student explanations for their 
\textit{convincingness} as judged by \textit{peers}.

We suggest that the results of our work can inform the design of TMPI platforms.
However, in a broader context, we aim to contribute to the growing body of 
research surrounding technology-mediated peer-review, specifically where 
learners do not provide holistic scores, but generate their evaluative 
judgments in a comparative setting. 
Such platforms will invariably have to deal with at least three issues, which 
our work helps to address.

The first issue is about students: providing feedback to learners on the 
characteristics common to the most convincing arguments in their discipline, 
promotes learning and the development of critical reasoning skills.
The second issue is in providing support to teachers: in such platforms, the 
amount of data generated scales very quickly.
The data associated with each student-item pair includes many relevant 
variables: correct answer choice on first attempt, student explanation, subset 
of explanations shown, time spent writing and reading explanations, correct 
answer on second attempt, and the peer-explanation chosen as most convincing 
(see figure \ref{fig:make_pairs_a}).   
This amount of information can be overwhelming for instructors who use such 
tools regularly as part of formative assessment. 
Automatically identifying the highest, and lowest, quality student 
explanations, as judged by other students, can support instructors in providing 
timely feedback. 
A third related issue is in maintaining the integrity of such platforms: 
automatic filtering of irrelevant/malicious student explanations is paramount, 
since they may be shown to future students \cite{gagnon_filtering_2019}, a 
non-trivial task for natural language content, without expensive expert 
moderation.

This paper begins with an overview of research related to learnersourcing,  and 
argument mining (section \ref{sec:related_work}).
We then describe our TMPI dataset, as well as publicly available reference 
datasets of argument quality, which we use to evaluate our methodology (section 
\ref{sec:datasets}).
Our most important contribution is in proposing a methodology for evaluating 
the quality of student explanations, along the dimension of 
\textit{convincingness}, in TMPI environments; we demonstrate this methodology 
in section \ref{sec:methodology}.
We then present our results on choosing the appropriate \textit{measure} of 
explanation convincingness (section \ref{sec:measure}), and finally, we 
describe how we \textit{model} these convincingness ``scores'' so as to 
identify the linguistic features of explanations most often associated with 
high-quality explanations (section \ref{sec:model}).

\section{Related Work}\label{sec:related_work}


\subsection{Automated Essay Scoring}
TO DO
taxonomy of argument quality \cite{wachsmuth_computational_2017}

\cite{persing_modeling_2015}

\cite{nguyen_argument_2018}


\subsection{Learnersourcing student explanations}
TMPI is a specific case of  
\textit{learnersourcing}\cite{weir_learnersourcing_2015}, wherein students first
generate content, and then help curate the content base, all as part of their 
own learning process.
Notable examples include PeerWise \cite{denny_peerwise:_2008} and RiPPLE 
\cite{khosravi_ripple_2019}, both of which have student generate learning 
resources, which are subsequently used and evaluated by peers as part of 
formative assessment activities.

One of the earliest efforts to leverage peer judgments of peer-written 
explanations specifically is from the AXIS system\cite{williams_axis:_2016}, 
wherein students solved a problem, provided an explanation for their answer, 
and evaluated explanations written by their peers.
Using a reinforcement-learning approach known as ``multi-armed bandits'', the 
system was able to select peer-written explanations that were rated as helpful 
as those written by an expert.
Our research follows from these studies in scaling to multiple domains, and 
focusing on how the vote data can be used more directly to model argument 
quality as judged by peers.

\subsection{Ranking Arguments for Quality}
Rank aggregation is the task of combining the preferences of multiple agents 
into a single representative ranked list.
It has long been understood that obtaining pairwise preference data may be 
less prone to error on the part of the annotator, as it is a simpler task than 
rating on scales with more gradations. 
(This is relevant in TMPI, since each student is choosing one explanation as 
the most convincing only in relation to the subset of others that are shown.)

A classical approach for aggregating pairwise preference data into a ranked 
list is using the Bradley-Terry model \cite{bradley_rank_1952}. 
This has been extended to incorporate the quality of contributions of different 
annotators in a crowdsourced setting when evaluating relative reading level in 
a pair passages \cite{chen_pairwise_2013}. 

When evaluating argument convincingness, one of the first approaches proposed 
is based on constructing an ``argument graph'', where a weighted edge is drawn 
from node A to node B for every pair where argument A is labelled as more 
convincing than argument B. 
After filtering example pairs that lead to cycles in the graph, PageRank scores 
are derived from this directed acyclic graph, and the PageRank 
scores of each argument are used as the gold-standard to rank for 
convincingness \cite{habernal_which_2016}.

More recently, a relatively simpler heuristic WinRate score has been shown to 
be a competitive alternative, wherein the rank score of an argument is simply 
the (normalized) number of times that argument has been chosen as more 
convincing in a pair, divided by the number of pairs it appears in 
\cite{potash_ranking_2019}.

Finally, a neural approach based on RankNet has recently yielded state of the 
art results by joining two Bidirectional Long-Short-Term Memory Networks in a 
Siamese architecture, and appending a softmax layer to the output, pairwise 
preferences and overall ranks were jointly modelled in publicly available 
datasets \cite{gleize_are_2019}.

We will explore two of these options as part of our methodology in our rank 
aggregation step, via several related methods: 
the probabilistic Bradley-Terry model, as well as two of its variants (CrowdBT 
and the Elo rating system), and the simple heuristic scoring model.
(We omit a neural approach, as model interpretability is key in the educational 
context,)


\section{Data}\label{sec:datasets}

\subsection{Argument Mining Datasets}
Much of our methodology is inspired by work on modelling argument quality along 
the dimension of \textit{convincingness}, as described in section 
\ref{sec:related_work}. 
In order to contextualize the performance of these methods in our educationale 
setting, we apply the same methods to publicly available datasets from the 
argument mining research community as well, and present the results. 
These datasets are described in table \ref{tab:data_summary}, alongside the 
TMPI data at the heart of our study. 
The \textbf{UKP} dataset\cite{habernal_which_2016} is the first set of labelled 
argument pairs to be released publicly.
Crowd-workers were presented with pairs of arguments on the same stance of a 
debate prompt, and were asked to choose which was more convincing.
The authors of the \textbf{IBM\_ArgQ} dataset\cite{toledo_automatic_2019} offer 
a similarly labelled, but much more tightly curated dataset, with strict 
controls on argument word count and relative difference in lengths in each pair.
This was partly in response to the observation that crowd labels could often be 
predicted simply by choosing the longer text from the pair.
The labelled argument pairs in the \textbf{IBM\_Evi} dataset 
\cite{gleize_are_2019} are actually generated by scraping Wikipedia, and the 
crowd workers were asked to choose the argument from the pair that provided the 
more compelling evidence in support of the debate stance.

As described above in our section on related work, these datasets were released 
not just with the labelled argument pairs, but holistic rank scores for each 
argument, that were each derived in different ways. 
We will be comparing our proposed \textit{measures} of convincingness to these 
rank scores.


\subsection{DALITE}
The central data for this study come from myDALITE.org, which is a hosted 
instance of an open-source project, 
\verb|dalite|\footnote{\url{https://github.com/SALTISES4/dalite-ng}}, 
maintained by a Canadian researcher-practitioner partnership focused on 
supporting teachers in the development of active learning pedagogy 
\href{saltise.ca}{SALTISE}.
The data comes from introductory level university science courses, and 
generally spans different teachers at different colleges and universities in 
Canada. 
The \textit{Ethics} dataset comes from a popular MOOC, wherein the TMPI prompts 
are slightly different from the \textit{Physics} and \textit{Chemistry} 
prompts, in that there is no ``correct'' answer choice, and that the goal is to 
have students choose a side of an argument, and justify their choice.
Table \ref{tab:data_summary} gives an overview of the datasets included in this 
study.


\begin{table}
	\input{data/df_summary_all_datasets}
	\caption{
		Summary statistics for reference datasets from argument mining research 
		community, and DALITE, a TMPI environment used mostly in undergraduate 
		science courses in Canada. 
		In the argument reference datasets \textit{topic} are debate prompts 
		shown to crowdsourcing workers (e.g. \textit{``social media does more 
		good than harm''}), while a \textit{topic} in DALITE is a question item.
		The explanations given by students are analagous to the ``arguments'',  
		which are then assembled into pairs based on what was shown, and 
		eventually chosen by each student.
		\textit{wc} is the average number of tokens in each 
		argument/explanation in each topic.
		All averaged quantities are followed by a standard deviation in 
		parentheses.
	}
	\label{tab:data_summary}
\end{table}

To stay consistent with the argument mining reference dataset terminology, we 
refer to a question-item as a ``topic''.
Student explanations from DALITE are divided up by the associated question item 
prompts.
The transformation of TMPI student explanations (``args'') into ``pairs'' is 
described in section \ref{sec:methodology}. 
The filtering of DALITE data is based on the following three steps:
\begin{enumerate}
	\item Many question items have been completed by several hundreds of 
	students. 
	As such, almost half of student explanations have only been shown by 
	another peer. 
	As such, we retain only those student answers that have been shown to at 
	least 5 other students.
	\item As a platform for formative assessment, not all instructors provide 
	credit for the explanations students write.
	As such, there are some students who do not put much effort into writing 
	good explanations.
	We filter out only those student answers that have at least 10 words.
	\item after the previous two steps, we include data only from those 
	questions that have at least 100 remaining student answers.
\end{enumerate}



\section{Methodology}\label{sec:methodology}

We borrow our methodological approach from research in argument mining (AM), 
specifically related to modelling argument quality along the dimension of 
\textit{convincingness}.
A common approach is to curate pairs of arguments made in defence of the same 
stance on the same topic.
These pairs are then presented to crowd-workers, whose task it is to label 
which of the two is more convincing. 
These pairwise comparisons can then be aggregated using rank-aggregation 
methods so as to produce a overall ranked list of arguments.
We extend this work to the domain of TMPI, and define prediction tasks that not 
only aim to validate this methodology, but help answer our specific research 
questions.

\subsection{Rank Aggregation}\label{sec:rank_agg}
The raw data emerging from a TMPI platform is tabular, in the form of 
student-item observations.
As shown in figure \ref{fig:make_pairs_a}(a), the fields include the item 
prompt, the student's \textit{first} answer choice, their accompanying 
explanation, the peer explanations shown on the review step, the student's 
\textit{second} answer choice, and the peer explanation they chose as most 
convincing (\verb|None| if they choose to ``stick to their own''), as well as 
timestamps for the first and second attempt.

It should be noted that there is no credit associated with which explanation is 
chosen in DALITE (all points are attributed based on the  correctness of the 
answer choice on the first and second steps).
After carefully looking at timestamp data, we observe that a large fraction of 
students who choose to ``stick to their own'', spend as little as 5 seconds on 
the review step.
For this reason, we exclude these students' data, and build all rank scores 
only based on students who explicitly choose a peer's explanation over their 
own.

After this first filtering step, we take the TMPI observations for each 
question, and construct explanation pairs, as in figure 
\ref{fig:make_pairs_a}(b).

\begin{figure}[H]
	\centering
	\def\svgscale{0.40}
	\input{img/make_pairs_a.pdf_tex}
	\caption{
	Example of student-item observations from a TMPI environment. 
	This figure follows from figure \protect\ref{fig:tmpi}.
	(a) Student $s_{30}$ chose the correct \textbf{D} as the answer on 
	their first attempt, and provided the explanation $e_{30}$ in the 
	dataset for this question. 
	The student is shown a subset of explanations from previous students for 
	\textbf{D}, as well as for \textbf{A} (the most popular incorrect 
	answer). 
	The student decides to keep the same answer choice \textbf{D}, and 
	indicates that the explanation $e_{25}$ is the most convincing.
	This is referred to as a \textit{Right}$\rightarrow$\textit{Right} 
	transition.
	(b) This observation is transformed into 8 explanation pairs. The first 
	pair is for the choice of $e_{25}$ over what the student wrote themselves, 
	and the other seven are for the choice of $e_{25}$ over the other shown 
	explanations. 
	The pairs are labelled as such that $e_{25}$ is the more convincing of the 
	pair. 
	(c) This pairwise preference data is aggregated global ranked list of 
	student explanations for this question, where each explanation is assigned 
	a real-valued rank score (using the methods described in section 
	\protect\ref{sec:rank_agg}).
}
\label{fig:make_pairs_a}
\end{figure}


\begin{enumerate}
	
	\item \textbf{WinRate}, defined as the ratio of times an explanation is 
	chosen to the number of times it was shown.
	
	\item \textbf{BT} score, which is the argument ``quality'' parameter 
	estimated for each explanation, according to the \textit{Bradley-Terry} 
	model, where the probability of argument A being chosen over argument B is 
	given by 
	
	$$
	P(a>b) = 
	\frac{1}{1+e^{\beta_b-\beta_a}}
	$$
	where $\beta_i$ is the latent strength parameter of argument $i$. 
	
	We decompose each student-item observation into argument pairs, where the 
	chosen explanation is paired with each of the other shown ones, and the 
	pair is labelled with $y=-/+1$, depending on whether the chosen explanation 
	is first/second in the pair.
	Assuming there are $N$ explanations, labelled by $K$ students, and $S_K$ 
	labelled pairs,the latent strength parameters are estimated by maximizing 
	the log-likelihood given by:
	$$
	\ell(\boldsymbol{\beta})=\sum_{K}\sum_{(i,j)\epsilon S_K}^{} 
	\log\frac{1}{1+e^{\beta_i - \beta_j}}
	$$
	subject to $\sum_{i}\beta_i=0$.
	
	
	\item The \textbf{Elo} rating system\cite{elo_rating_1978}, which was 
	originally proposed for ranking chess players, has been successfully used 
	in adaptive learning environments (see \cite{pelanek_applications_2016} for 
	a review). 
	This rating method can be seen as a heuristic re-parametrization of the 
	\textbf{BT} method above, where the probability of argument A being chosen 
	over argument B is given by
	$$
	P(a>b) = P_{ab} = \frac{1}{1+10^{(\beta_b-\beta_a)/400}}
	$$
	
	All arguments are initialized with an initial value of 1500 points, an the 
	rating of any argument is only updated after it appears in a pairwise 
	comparison with another.
	The rating update rule transfers points from the winner, to the loser, in 
	proportion to the difference in strength:
	
	$$
	\beta_a':=\beta_a+K(P_{ab} - \beta_a)
	$$
	
	While the \textbf{BT} model can be thought of a \textit{consensus} 
	approach, \textbf{Elo} ratings are dynamic and implicitly give more weight 
	to recent data\cite{aldous_elo_2017}.
	
%	\item \textbf{Crowd-BT} \cite{chen_pairwise_2013} is an extension of the 
%	\textbf{BT} model, tailored to settings where different annotators may have 
%	assigned opposite labels to the same pairs, and the reliability of each 
%	annotator may vary significantly. 
%	A reliability parameter is estimated for each student, 
%	
%	$$
%	\eta_k \equiv P(a >_k b | a >b )
%	$$
%	
%	where $\eta_k \approx 1$ if the student  $k$ agrees with most other 
%	students, and $\eta_k \approx 0$ if the student is in opposition to their 
%	peers.
%	This changes the model of argument $a$ being chosen over $b$ by student $k$ 
%	to 
%	$$
%	P(a >_k b) = 
%	\eta_k \frac{e^{\beta_a}}{e^{\beta_a}+e^{\beta_b}} + (1-\eta_k) 
%	\frac{e^{\beta_b}}{e^{\beta_a}+e^{\beta_b}}
%	$$
%	and the log-likelilood maximized for estimation to 
%	
%	$$
%	\ell(\boldsymbol{\eta},\boldsymbol{\beta})=\sum_{K}\sum_{(i,j)\epsilon 
%		S_K}^{} 
%	\log \left[ \eta_k \frac{e^{\beta_a}}{e^{\beta_a}+e^{\beta_b}} + (1-\eta_k) 
%	\frac{e^{\beta_b}}{e^{\beta_a}+e^{\beta_b}} \right]
%	$$
%	
\end{enumerate}


\begin{figure}[H]
	\scalebox{0.7}{\input{img/corr_to_reference_score.pgf}}
	\caption{Distribution of Spearman correlation  coefficients measured 
	between ``reference'' rank scores, and the rank aggregation methods 
	(WinRate, BT, Elo) used in our proposed methodology, across the different 
	topics of the reference argument mining datasets.}
	\label{fig:corr_to_reference_score}
\end{figure}

In order to evaluate these rank aggregation different scores, and address 
our research question, we employ a time-series based cross-validation scheme:
at each timestep, we calculate the aggregated argument \textit{convincingness} 
scores from past students, and set out to predict: which arguments will be 
chosen as more convincing from the pairs constructed for the current student?  



\begin{figure}
	\centering
	\def\svgscale{0.5}
	\input{img/make_pairs_time_series.pdf_tex}
	\caption{
		All of the pairs constructed for answers by students $i<=30$ are used 
		to learn rank scores $\sigma$, which are evaluated in their ability to 
		predict the label in the held out test set: argument pairs constructed 
		from the answer of student $i=31$.
		We must exclude those pairs which include the 31st student, as this 
		explanation is unseen in the training set, and does not have a rank 
		score 
	}
	\label{fig:make_pairs_time_series}
\end{figure}

\begin{figure}
	\centering
	\def\svgscale{0.5}
	\input{img/rankingreliability.pdf_tex}
	\caption{
		Validation scheme for evaluating reliability of rankings.
		At each time step $\tau$ we take all students $i<=\tau$, and split 
		student into two batches (chosen at alternating time steps)
		We learn rankings for each of these batches, and evaluate the Kendall 
		Tau rank correlation as estimate of reliability of the rankings.
	}
	\label{fig:rank_reliability}
\end{figure}

\begin{figure}
	\scalebox{0.4}{\includegraphics{img/evaluate_rankings.png}}
	\caption{Methodology for evaluation of rank scores}
	\label{fig:evaluate_rankings}
\end{figure}

\section{Measuring Argument Quality}\label{sec:measure}



\begin{figure}
	\scalebox{0.6}{\input{img/corr_plot.pgf}}
	\caption{
		Correlation between different Ranking Scores for each explanation, 
		disaggregated by transition type  
	}
	\label{fig:acc_by_rank_score_type}
\end{figure}

\textbf{TO DO}


%\begin{figure}
%	\scalebox{0.6}{\input{img/acc_by_rank_score_type.pgf}}
%	\caption{
%		Comparing the classification accuracy of different rank aggregation 
%		scores in predicting which argument is more convincing from a pair, 
%		under a time-series cross-validation scheme. Data is averaged across 
%		all questions and times steps, for different disciplinary subsets of 
%		TMPI data. 
%	}
%	\label{fig:acc_by_rank_score_type}
%\end{figure}

\begin{figure}
	\scalebox{0.6}{\input{img/acc_by_transition.pgf}}
	\caption{
		Comparing the classification accuracy of different rank aggregation 
		scores in predicting which argument is more convincing from a pair. 
		Rank scores are calculated with the vote data of half the students, and 
		tested on the pairs generated by the other half. 
		Data is averaged across all questions, dis-aggregated by different TMPI 
		transition types. 
	}
	\label{fig:acc_by_rank_score_type}
\end{figure}

\begin{figure}
	\scalebox{0.6}{\input{img/corr_by_batch.pgf}}
	\caption{
		Pearson correlation coefficient between different rank score types, 
		derived from two independent groups of students, averaged over all 
		questions, dis-aggregated by different TMPI transition types.
	}
	\label{fig:acc_by_rank_score_type}
\end{figure}


%\begin{figure}
%	\includegraphics[width=\linewidth]{img/acc_vs_time_Physics_ada.png}
%	\caption{to do}
%	\label{fig:acc_vs_time}
%\end{figure}




\section{Modelling Argument Quality Scores}\label{sec:model}

TO DO

Cross-topic validation scheme; control for word-count

\begin{figure}
	\scalebox{0.5}{\input{img/prec_at_K.pgf}}
	\caption{
		Evaluating of regression models tasked with predicting 
		\textit{convincingness} score of arguments/TMPI-explanations based on 
		linguistic features.
		Evaluation metric is \textbf{precision @ K}, where we verify how many 
		of the predicted top-K ranked explanations are in the measured top-K 
		list (using ``winrate'' as a \textit{measure} of convincingness). 
		Precision is plotted against the size of the test-set on the horizontal 
		axis, under a cross-topic validation scheme.
		The dashed line is a (harsh) approximation of the probability of 
		choosing the top-K explantions purely by chance (K/N, which is much 
		greater than ``N choose K'').
		Each dot represents the performance on one held-out 
		topic/TMPI-question-prompt, color-coded based on which 
		dataset/discipline it originates from.
		We compare Linear Regression with a Decision Tree Regressor in the two 
		columns.
	}
	\label{fig:prec_at_K}
\end{figure}

The goal \textbf{RQ1} is establish which rank aggregation methods are best 
suited for the context of TMPI, such that one can take the comparative 
preference data from many students who each see different subsets of peer 
explanations.
We build on the results from the previous section to now predict these 
aggregate scores for each explanation, using 
linguistic properties of those explanations



We address \textbf{RQ2} with a regression task of predicting the argument 
\textit{convincingness} scores via a feature-rich document vector.

%We experiment with vector space models with different document representations:
%\begin{enumerate}
%	\item LSA vectors (10,50,100 components) \cite{deerwester_indexing_1990}
%	\item Glove embeddings \cite{pennington_glove:_2014}
%	\item BERT embeddings \cite{devlin_bert_2018}, out-of-the-box, and 
%	fine-tuned for the current classification task
%\end{enumerate}

The advantage of a feature-rich approach lies in the interpretability for 
teachers in their reporting tools, as well as generalizability to new items 
before vote data can be collected.
The list of features included here are derived from related work in argument 
mining \cite{habernal_which_2016}\cite{persing_end--end_2016}on student essays, 
automatic short answer scoring \cite{mohler_text--text_2009}

\begin{itemize}
	
	\item Surface Features: 
	word count,
	sentence count, 
	max/mean word length, 
	max/mean sentence length;
	
	\item Lexical: 
	uni-grams \& bigrams, 
	type-token ratio, 
	number of keywords (defined by open-source discipline specific 
	text-book), 
	number of equations;
	
	\item Syntactic: 
	POS n-grams (e.g. \textit{nouns, prepositions, 
		verbs,conjunctions,negation, adjectives, adverbs, punctuation}), 
	modal verbs (e.g. \textit{must, should, can, might}),
	contextuality/formality measure \cite{heylighen_variation_2002},
	dependency tree depth;
	
	\item Semantic:
	using LSA vectors trained on domain specific corpora, in this case an 
	open-source textbook in the discipline, we calculate similarity to all 
	other explanations in LSA space;
	
	\item Co-reference \cite{persing_end--end_2016}: 
	fraction of entities from the prompt mentioned in each sentence, 
	averaged over all sentences (using neural Co-reference resolution)
	vector cosine similarity between student explanation and prompt, 
	and answer choices; 
	
	\item Readability:
	Fleish-Kincaid,
	Coleman-Liau,
	spelling errors
\end{itemize}


Features typical to NLP analyses in Learning Analytics that are not included 
here are cohesion, sentiment, and psycholinguistic features.


% REMOVE NOCITE OR IT WILL LIST EVERYTHING IN YOUR DATABASE AS A REFERENCE
%\nocite{*}

\bibliographystyle{acmtrans}
\bibliography{MyLibrary}

\end{document}