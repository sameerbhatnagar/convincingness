% This is samplepaper.tex, a sample chapter demonstrating the
% LLNCS macro package for Springer Computer Science proceedings;
% Version 2.20 of 2017/10/04
%
\documentclass[runningheads]{llncs}
%

\usepackage{hyperref}
\usepackage{url}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{pgf}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}




\begin{document}
%
\title{Automatic Explanation Quality Assessment in Online Learning 
Environments: new datasets and methods}
\titlerunning{Convincingness \& Peer Instruction}
%
\author{Sameer Bhatnagar\inst{1} \and
Amal Zouaq\inst{1} \and
Michel C. Desmarais\inst{1} \and
Elizabeth Charles\inst{2}
}
%
\authorrunning{S. Bhatnagar et al.}

\institute{Ecole Polytechnique Montreal 
\email{\{sameer.bhatnagar,amal.zouaq,michel.desmarais\}@polymtl.ca}\and
Dawson College
\email{echarles@dawsoncollege.qc.ca}\\
}
%
\maketitle              % typeset the header of the contribution
%
\begin{abstract}
\textit{Asynchronous Peer Instruction} is an increasingly popular item type in 
online learning environments. Students first respond to a question item, but 
they must also provide an explanation for their reasoning. 
They are then presented alternative explanations as written by their peers, and 
given the opportunity to change their initial answer choice, based on those 
they find most convincing. 
The peer-explanations that students find most convincing represent valuable 
data, for teachers to better grasp their students' understanding, and for the 
learning environment itself, as higher quality explanations can be shown to 
students as examples to compare their work to. 
This study reports on the application of argument mining methods in the 
context of asynchronous peer instruction, with the objective of automatically 
identifying high quality student explanations. 
Our results offer the potential to inform the design of ``learnersourcing'' 
systems, where the content is both \textit{generated} and \textit{evaluated} by 
novices. 
These design choices are critical as these systems scale, especially with 
respect to providing pedagogically insightful reports to teachers, and 
presenting engaging alternative explanations to students to promote higher 
order thinking. 

\keywords{Argument mining  \and Learnersourcing \and Peer Instruction}
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}
As online learning environments scale in the number of items, providing 
immediate and tailored feedback to students becomes intractable for activities 
that prompt for open ended responses. \textit{Peer assessment} and \textit{peer 
feedback} can address this issue, and have the added benefit of being an 
effective way to learn for the student who give the feedback as 
well\cite{jhangiani_impact_2016}. 
However the drawback of such approaches often lie in the varying ability of 
novices to provide good feedback to their peers.
Frameworks such as Adaptive Comparative Judgement\cite{pollitt_method_2012}, 
where teachers assess student submissions by simply choosing which is better 
from a pair,have been shown to be a reliable and valid alternative to absolute 
grading.
Thus it is not surprising that there are a growing number of online learning 
environments that extend this paradigm to students with \textit{comparative 
peer assessment}, wherein, after students submit their own work, they are asked 
to compare and contrast a pair of two of their peers' submissions, and provide 
feedback
\cite{univeristy_of_british_columbia_ubc/ubcpi_2019}
\cite{cambre_juxtapeer:_2018}
\cite{potter_compair:_2017}.

A subset of these tools then use the comparative peer assessment data to then 
inform the choice of how the pairs are constructed, and which students are 
assigned to which pairs,
\cite{khosravi_ripple_2019}
\cite{williams_axis:_2016}
\cite{saltise_saltises4/dalite-ng_2019},
engaged in the classic trade-off from the field reinforcement learning: 
exploiting the student submissions for which we have reliable data as to their 
quality, while exploring student work that is newer to the database, and needs 
to be shown and evaluated in order to get an estimate of its quality.

This is especially crucial in systems where the content generated by students 
is actually a central part of the pedagogical script: in platforms where 
students generate explanations, hints, or even new question items, which are 
then shown to future students, the ability to automatically identify the high 
quality content is critical. 
We focus our attention on learning environments that enable \textit{peer 
instruction}\cite{crouch_peer_2001}, which are built on a two-stage script: (i) 
students are prompted to answer a multiple choice question, and provide a 
free-text explanation that justifies their answer; (ii) without revealing the 
correct answer, students are prompted to reconsider their answer, by presenting 
them a selection of explanations written by previous 
students\cite{charles-woods_designing_nodate}.

We frame these explanations to multiple choice questions as \textit{arguments} 
meant to persuade one's peers. As such, we explore methods from the 
\textit{argument mining} research community, where there has been a growing 
body of work on automatically assessing argument quality, along the dimension 
of \textit{convincingness}.
The objective of this study is to determine whether we can automatically assess 
the \textit{convincingness} of student explanations in peer-instruction based 
learning environments, based on pairwise preference data generated by students 
in undergraduate level science courses.  


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}
\subsection{Learnersourcing \& Comparative Peer Assessment}
Ripple\cite{khosravi_ripple_2019}, 
AXIS\cite{williams_axis:_2016}
Juxtapeer\cite{cambre_juxtapeer:_2018}, 
ComPAIR\cite{potter_compair:_2017}
UBCPI\cite{univeristy_of_british_columbia_ubc/ubcpi_2019}
Peerwise\cite{denny_peerwise:_2008}
\subsection{Argument Quality \& Convincingness}
Conventional argument-mining pipelines include several successive components, 
starting with the automatic detection of argumentative units, classification of 
these units into types (e.g. major claim, minor claim, premise), and 
identification of argumentative relations (which evidence units support which 
claim). Such pipelines are essential in question-answering systems 
\cite{lippi_argumentation_2016} and are at the heart if the IBM Project Debater 
initiative. 

Work in the area of automatic evaluation of argument quality finds its roots in 
 detecting evidence in legal texts\cite{moens_automatic_2007}, but has 
 accelerated in recent years as more datasets become 
available in everyday contexts, and focus shifts to modelling more qualitative 
measures, such as \textit{convincingness}. 

Some of the earlier efforts included work on automatically 
scoring of persuasive essays \cite{persing_end--end_2016} and modelling 
persuasiveness in online debate forums \cite{tan_winning_2016}. However, 
evaluating argument \textit{convincingness} with an absolute score can be 
challenging, which has led to significant work in adopting a pairwise approach, 
where data consists of pairwise observations of two arguments, labelled with 
which of the two is most convincing.

%\begin{table}

	\input{publication_artefacts/data/sample_obs_UKP}
	\input{publication_artefacts/data/sample_obs_IBM}
	
%	\caption{Example of instance in pairwise comparison task, where two 
%		students explanations are compared, and one is chosen as more 
%		convincing}
	
%	\label{tab:sample_obs}
%\end{table}

In \cite{habernal_which_2016}, the authors propose a feature-rich support 
vector machine, as well as an end-to-end neural approach based on pre-trained 
Glove vectors and a bidirectional Long-Short-Term Memory network for the 
pairwise classification task. This is extended in \cite{gleize_are_2019}, where 
the authors build a Siamese network architecture, where each leg is a BiLSTM, 
taking as input the pair of explanations as Glove embeddings 
\cite{pennington_glove:_2014}, in order to detect which of argument in a pair 
has the most convincing evidence. Finally, based on the success of transformer 
models such as BERT\cite{devlin_bert_2018}, the authors of 
\cite{toledo_automatic_2019} 
release a dataset of argument pairs and show that these models accurately 
predict the most convincing argument in a pair.



\begin{itemize}

	\item \cite{toledo_automatic_2019} Assessment of argument quality, with a 
	dataset that has both individual scores 
	and pairwise-ranked data 
	
\end{itemize}
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
\section{Data}

\begin{table}
	\caption{Descriptive statistic for each dataset of argument pairs, with 
		last rows showing \textit{dalite} split by discipline}
	\input{publication_artefacts/data/df_summary_final}
	\label{tab:data_summary}
\end{table}

One of the objectives of this study is to compare and contrast how argument 
mining methods for evaluating argument quality, specifically for argument 
\textit{convincingness}, perform in an online learning environment with 
learner-generated and annotated arguments. Along with myDALITE data, we include 
in our study two publicly 
available datasets, each specifically curated for the task of automatic 
assessment of argument quality along the dimension \textit{convincingness}. 
Table \ref{tab:data_summary} summarizes some of the descriptive statistics that 
can be used to compare these sources, and potentially explain some of our 
experimental results.

\subsection{UKP \& IBM}
\textbf{UKPConvArgStrict}\cite{habernal_which_2016}, hence forth referred to as 
\textbf{UKP}, was the first to propos the task of pairwise preference learning 
for argument convincingness. The dataset consists of just over 1k individual 
arguments, that support a stance for one of 16 topics. These arguments were 
distributed as 11.6k pairs to annotators on a crowd-sourcing platform, where 
the task was to choose which of the two arguments, for the same stance 
regarding the same topic, was more convincing. More recently, a second similar 
dataset, \textbf{IBMArgQ-9.1kPairs}\cite{toledo_automatic_2019}, henceforth 
referred to as \textbf{IBM}, which is made of 3.4k individual arguments for 11 
topics, assembled into 9.1k pairs labelled for which is more convincing. One of 
the key differences between these two is that \textbf{IBM} data is more 
strongly curated with respect to the relative length of the arguments in each 
pair: in order to control for the possibility that annotators may make their 
choice of which argument in the pair is more \textit{convincing} based merely 
on the length of the text, the mean difference in word count, $\overline{\Delta 
wc}$, is just 3 words across the entire dataset, which is 10 times more 
homogeneous than pairs in \textbf{UKP}.



\subsection{myDALITE}
The dataset is comprised of pairs of student explanations for a particular 
answer choice to a given question. 
The first explanation is always the one written by the learner-annotator, while 
the second is an alternative which they either chose as more convincing, or 
not. 
To ensure internal reliability, we only keep explanations that were chosen at 
least \input{publication_artefacts/data/VOTES_MIN}times. 
To ensure that the explanations in each pair are of comparable length, we keep 
only those with word counts that are within 
\input{publication_artefacts/data/MAX_WORD_COUNT_DIFF}words of each other. 

This leaves us a dataset with \input{publication_artefacts/data/N}observations, 
spanning \input{publication_artefacts/data/n_students}learner annotators having 
completed, on average, 
\input{publication_artefacts/data/avg_q_per_student}items each, from a total of 
\input{publication_artefacts/data/n_questions}items across three disciplines, 
with at least 
\input{publication_artefacts/data/MIN_RECORDS_PER_QUESTION}explanation-pairs 
per item.


\begin{table}
	\caption{Observations of students choosing a peer explanation as 
		more 
		convincing than their own, or not, aggregated by discipline and whether 
		they 
		started and finished with the correct answer}
	
	\input{publication_artefacts/data/transitions_by_discipline}
	
	\label{tab:transitions_by_discipline}
\end{table}

Table \ref{tab:transitions_by_discipline} highlights one key difference between 
the modelling task of this study, and related work in argument mining, where 
annotators are presented pairs of arguments that are always for the same 
stance, in order to limit bias due to their opinion on the motion when 
evaluating which argument is more convincing.
In a \textit{Peer Instruction} learning environment, other pairings are 
possible and pedagogically relevant. 
In this dataset, the majority of students keep the same answer choice between 
the two steps of the prompt, and so they are comparing two explanations that 
are either both correct (\textit{``rr''}) or incorrect (\textit{``wr''}). 
However, there is \input{publication_artefacts/data/frac_switch}\% of the 
observations in this dataset are for students who not only choose an 
explanation more convincing than their own, but also switch answer choice, 
either from the incorrect to correct, or the reverse. 
These pairs add a different level of complexity to the model, but are very 
pertinent in the pedagogical context: what are the argumentative features which 
can help students remediate an initial wrong answer choice (\textit{``wr''})?
What are the features that might be responsible for getting students to 
actually move away from the correct answer choice (\textit{``rw''})?



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methods}
Choosing which argument is more convincing from a pair is an 
ordinal regression problem, where the 


In pairwise classification tasks so 50\% is the baseline performance.  
In Table \ref{tab:baselines} we begin by comparing the \textit{ArgLongest} 
model baseline, where we predict that students simply choose the longer 
explanation of the pair.
We also include two baseline models on \textit{Bag of Words} models: 
\textit{ArgBoWGen} where the term-document-matrix is built from an open-source 
textbook from the corresponding discipline\footnote{https://openstax.org/}, and 
\textit{ArgBoWItemSpec}, where the term-document matrix is built from the words 
students have used for the item (pertinent when no reference text is available 
for a discipline).
As this is a new context for these argument mining methods, we include the same 
baselines on the carefully curated \textit{IBMArgPair} dataset, which is of the 
same format.



For our experiments, we begin by following the line of work proposed by 
\cite{habernal_which_2016}, and experiment with a feature-rich linear SVM 
classifier for the pairwise classification task. We use a similar feature set, 
which we categorize as \textbf{lexical}, \textbf{syntactic}, and 
\textbf{semantic}, as described in Table \ref{tab:features}. we begin by 
computing the feature vector for each explanation, and compute the difference 
for each pairwise ranking instance as per the well established SVM-Rank 
algorithms \cite{joachims_optimizing_2002}, training the model to learn which 
of the pair is the more convincing argument. 



\subsection{SVM-Rank}
It has been proven that a binary ordinal regression problem, can be cast into 
an equivalent binary classification problem, wherein the model is trained on 
the \textit{difference} of the feature vectors of each argument in the pair 
\cite{herbrich_support_1999}. Referred to as \textit{SVM-rank}, this method of 
learning pairwise preferences has been used extensively in the context of 
information retrieval (e.g. ranking search results for a query based on past 
clickthrough data) \cite{joachims_optimizing_2002}, but also more recently in 
evaluating the journalistic quality of newspaper and magazine articles 
\cite{louis_what_2013}. We build and evaluate one such model, \textit{ArgBoW}, 
which takes as its input the words which were used in all of the arguments for 
a particular topic, regardless of stance. 

\subsection{BERT}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}


\begin{figure}
	\begin{subfigure}[t]{0.5\linewidth}
		\centering
		\scalebox{0.5}{\input{publication_artefacts/img/acc.pgf}}

		\caption{Pairwise ranking accuracy for different models across datasets}
	\end{subfigure}%
	\qquad
	\begin{subfigure}[t]{0.5\linewidth}
		\centering
		\scalebox{0.5}{\input{publication_artefacts/img/AUC.pgf}}

		\caption{Pairwise ranking classification ROC-AUC for different models 
		across datasets}
	\end{subfigure}
\end{figure}


\begin{figure}
	\begin{subfigure}[t]{0.5\linewidth}
		\centering
		\scalebox{0.5}{\input{publication_artefacts/img/acc_dalite.pgf}}
		
		\caption{Pairwise ranking accuracy for different models in myDalite 
		dataset, across disciplines}
	\end{subfigure}%
	\qquad
	\begin{subfigure}[t]{0.5\linewidth}
		\centering
		\scalebox{0.5}{\input{publication_artefacts/img/AUC_dalite.pgf}}
		
		\caption{Pairwise ranking classification ROC-AUC for different models 
		in myDalite dataset, across disciplines}
	\end{subfigure}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

In \cite{louis_what_2013}, for task of pairwise ranking of newspaper articles 
based on ``quality'', the authors achieve a similar result: when comparing the 
performance of SVM-rank models using different input feature sets (e.g. 
\textit{use of visual language}, \textit{use of named entities}, 
\textit{affective content}), their top performing models achieve ``same-topic'' 
pairwise ranking accuracy of 0.84 using a combination of content and writing 
features, but also a 0.82 accuracy with the content words as features alone.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future Work}
\cite{nguyen_computational_2015} and \cite{louis_what_2013} use combination of 
writing and content (BoW) features to achieve their best results, and thus is 
avenue must be explored more thoroughly, especially as this mayb avry across 
disciplines an teaching contexts.

In this study we do not ever infer which are, overall, the most convincing 
student explanations for any given item. Inferring a gold standard of global 
rankings, starting from these pairwise preference data can be accomplished 
using research from the information retrieval 
community\cite{chen_pairwise_2013}. Work on deriving point wise scores for 
argument pairs is proposed as a Gaussian Process Preference Learning task by 
\cite{simpson_finding_2018}. Seeing the lack of pointwise labels for overall 
convincingness, \cite{toledo_automatic_2019} released a dataset where they 
collect this data as well. A comparable source of data inside the myDALITE 
platform are the feedback scores teachers can optionally provide to students on 
their explanations.


 \bibliographystyle{splncs04}
 \bibliography{MyLibrary}
\end{document}

%%%%%%%%%%%%%%%%%%
% ARCHIVE
%
%\begin{table}
%\begin{tabular}{ |l|l|l|}
%	\hline
%	Feature Type & Feature & $\tau$ \\
%	\hline
%	\multirow{6}{*}{Lexical} 
%	& Uni+BiGrams & \\
%	& Spelling Errors &  \\
%	& Equations & \\
%	& Type Token Ratio &  \\
%	& Punctuation &\\
%	& Readability scores&\\
%	\hline
%	\multirow{5}{*}{Syntactic} 
%	& PoS Uni+Bigrams &   \\
%	& Dependancy Tree Depth  & \\
%	& Conjunctions  & \\ 
%	& Modal Verbs & \\ 
%	& Tree Production Rules &\\
%	\hline
%	\multirow{3}{*}{Semantic} 
%	& LSA Similarity &  \\
%	& LSA Similarity Question & \\
%	& Likelihood (Textbook) & \\ 
%	\hline
%\end{tabular}
%
%\caption{Features used in experiments with Linear SVM, annotated with 
%	kendall $\tau$ correlation and sigificance with target label}
%\label{tab:features}
%\end{table}




%\begin{figure}
%	\subfloat[Baseline models on myDalite]
%	{	
%	\input{data/results_overall_myDalite}
%    }\hfill
%	\subfloat[Baselines on IBM argument ranking dataset]
%	{	
%	\input{data/results_overall_IBMPairs}
%    }
%	\label{tab:baselines}
%\end{figure}

%\begin{figure}
%	\subfloat[BERT model, fine tuned on myDalite data, by discipline]
%	{	
%		\input{data/BERT_dalite}
%	}\hfill
%	\subfloat[BERT model performance on Biology data, by answer transition]
%	{	
%		\input{data/BERT_Bio_transition}
%	}
%	\label{tab:BERT_dalite}
%\end{figure}

%\begin{figure}
%	\subfloat[BERT model, fine tuned on myDalite data, by discipline]
%	{	
%		\input{data/BERT_dalite}
%	}\hfill
%	\subfloat[BERT model performance on Biology data, by answer transition]
%	{	
%		\input{data/BERT_Bio_transition}
%	}
%	\label{tab:BERT_dalite}
%\end{figure}