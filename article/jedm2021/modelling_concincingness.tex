
\documentclass[notitlepage,12pt]{jedm}
%\usepackage[sc,sf,small]{titlesec}
\usepackage[table]{xcolor}
\usepackage{url}
\usepackage{hyperref}
\hypersetup{
	colorlinks   = true, %Colours links instead of ugly boxes
	urlcolor     = blue, %Colour for external hyperlinks
	linkcolor    = blue, %Colour of internal links
	citecolor   = blue %Colour of citations
}

%-----------------------------------------------------------------------
% FINAL COPYEDITTED SUBMISSION - UNCOMMENT THIS TO SUPPRESS PAGE NUMBERS
%\pagenumbering{gobble}
%-----------------------------------------------------------------------

\begin{document}
	
	\title{Modelling Argument Quality in Technology Mediated Peer Instruction}
	\date{} %do not delete this, it suppresses insertion of the date
	
	\author{
		{\large Sameer Bhatnagar}
		\\Polytechnique Montreal

	 	\and 
	 	{\large Antoine Lefebvre-Brossard}
	 	\\Polytechnique Montreal

	 	\and 
	 	{\large Michel C. Desmarais}
	 	\\Polytechnique Montreal
	 	\and 
	 	{\large Amal Zouaq}
 		\\Polytechnique Montreal
 }

	
	\maketitle
	
	\begin{abstract}
		TO DO
		\\ %Keep \\ for spacing to keywords
		
		{\parindent0pt
			\textbf{Keywords:} 
		}
	\end{abstract}

\section{Introduction}

\section{Modelling Argument Quality Scores}
The goal \textbf{RQ1} is establish which rank aggregation methods are best 
suited for the context of TMPI, such that one can take the comparative 
preference data from many students who each see different subsets of peer 
explanations.
We build on the results from the previous section to now predict these 
aggregate scores for each explanation, using 
linguistic properties of those explanations



We address \textbf{RQ2} with a regression task of predicting the argument 
\textit{convincingness} scores using two different approaches to representing 
the student text: as an embedding inside a vector space models, or via a 
feature-rich document vector.

We experiment with vector space models with different document representations:
\begin{enumerate}
	\item LSA vectors (10,50,100 components) \cite{deerwester_indexing_1990}
	\item Glove embeddings \cite{pennington_glove:_2014}
	\item BERT embeddings \cite{devlin_bert_2018}, out-of-the-box, and 
	fine-tuned for the current classification task
\end{enumerate}

The advantage of a feature-rich approach lies in the interpretability for 
teachers in their reporting tools, as well as generalizability to new items 
before vote data can be collected.
The list of features included here are derived from related work in argument 
mining \cite{habernal_which_2016}\cite{persing_end--end_2016}on student essays, 
automatic short answer scoring \cite{mohler_text--text_2009}

\begin{itemize}
	
	\item Surface Features: 
	word count,
	sentence count, 
	max/mean word length, 
	max/mean sentence length;
	
	\item Lexical: 
	uni-grams \& bigrams, 
	type-token ratio, 
	number of keywords (defined by open-source discipline specific 
	text-book), 
	number of equations;
	
	\item Syntactic: 
	POS n-grams (e.g. \textit{nouns, prepositions, 
		verbs,conjunctions,negation, adjectives, adverbs, punctuation}), 
	modal verbs (e.g. \textit{must, should, can, might}),
	contextuality/formality measure \cite{heylighen_variation_2002},
	dependency tree depth;
	
	\item Semantic:
	using LSA vectors trained on domain specific corpora, in this case an 
	open-source textbook in the discipline, we calculate similarity to all 
	other explanations in LSA space;
	
	\item Co-reference \cite{persing_end--end_2016}: 
	fraction of entities from the prompt mentioned in each sentence, 
	averaged over all sentences (using neural Co-reference resolution)
	vector cosine similarity between student explanation and prompt, 
	and answer choices; 
	
	\item Readability:
	Fleish-Kincaid,
	Coleman-Liau,
	spelling errors
\end{itemize}


Features typical to NLP analyses in Learning Analytics that are not included 
here are cohesion, sentiment, and psycholinguistic features.


% REMOVE NOCITE OR IT WILL LIST EVERYTHING IN YOUR DATABASE AS A REFERENCE
%\nocite{*}

\bibliographystyle{acmtrans}
\bibliography{MyLibrary}

\end{document}